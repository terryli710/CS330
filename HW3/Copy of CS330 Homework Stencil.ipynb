{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Copy of CS330 Homework Stencil.ipynb","provenance":[{"file_id":"1uYqzq4Ix91DD4QdElyrbC_If0TKDf_3t","timestamp":1603385112627},{"file_id":"1VDBw9P1Bs1Xpzp8B3mxmQAdcXrh15yOZ","timestamp":1602965367864}],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"fwmQwFeQo2qW"},"source":["## CS 330 Homework 3 Installation\n","\n","The following code blocks will install the required libraries.\n"]},{"cell_type":"markdown","metadata":{"id":"EB7xAlJMrHwA"},"source":["## Setup for Google Drive and Required Libraries\n"]},{"cell_type":"code","metadata":{"id":"pa0Ri_edrNIT","cellView":"both","executionInfo":{"status":"ok","timestamp":1603647163658,"user_tz":420,"elapsed":18529,"user":{"displayName":"Yiheng Li","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ggo9FSuWqtcs815uMwgcd6czNNqCSqb_lRojxQg=s64","userId":"15227785711110309391"}},"outputId":"73c62c21-0ffe-437b-fbb3-c5f5b99e5e3e","colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["#@title Mount Google Drive\n","#@markdown Your work will be stored in a folder called `cs330_fall2020` by default to prevent Colab instance timeouts \n","#@markdown from deleting your edits and requiring you to redownload the mujoco library. Feel free to use this if you want to write out plots.\n","\n","import os\n","from google.colab import drive\n","drive.mount('/content/gdrive')\n","\n","#@title set up mount symlink\n","\n","DRIVE_PATH = '/content/gdrive/My\\ Drive/cs330_fall2020'\n","DRIVE_PYTHON_PATH = DRIVE_PATH.replace('\\\\', '')\n","if not os.path.exists(DRIVE_PYTHON_PATH):\n","  %mkdir $DRIVE_PATH\n","\n","## the space in `My Drive` causes some issues,\n","## make a symlink to avoid this\n","SYM_PATH = '/content/cs330_fall2020'\n","if not os.path.exists(SYM_PATH):\n","  !ln -s $DRIVE_PATH $SYM_PATH"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Mounted at /content/gdrive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"f-raFHMIpUun","executionInfo":{"status":"ok","timestamp":1603647198877,"user_tz":420,"elapsed":34652,"user":{"displayName":"Yiheng Li","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ggo9FSuWqtcs815uMwgcd6czNNqCSqb_lRojxQg=s64","userId":"15227785711110309391"}},"outputId":"05f17f24-05da-4930-ec97-9dc84c66497c","colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["#@title Install Requirements\n","#@markdown Requirements for the assignment and display drivers\n","\n","# Robot sim\n","!pip install gym==0.15.4\n","!pip install pygame\n","\n","# Various things for render\n","!apt-get install python-opengl -y\n","!apt install xvfb -y\n","\n","# Rendering Environment\n","!pip install pyvirtualdisplay\n","!pip install piglet\n","!sudo apt-get install -y xvfb ffmpeg\n","!pip install imageio\n","!pip install PILLOW"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Collecting gym==0.15.4\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1d/85/a7a462d7796f097027d60f9a62b4e17a0a94dcf12ac2a9f9a913333b11a6/gym-0.15.4.tar.gz (1.6MB)\n","\u001b[K     |████████████████████████████████| 1.6MB 4.5MB/s \n","\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from gym==0.15.4) (1.4.1)\n","Requirement already satisfied: numpy>=1.10.4 in /usr/local/lib/python3.6/dist-packages (from gym==0.15.4) (1.18.5)\n","Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from gym==0.15.4) (1.15.0)\n","Collecting pyglet<=1.3.2,>=1.2.0\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1c/fc/dad5eaaab68f0c21e2f906a94ddb98175662cc5a654eee404d59554ce0fa/pyglet-1.3.2-py2.py3-none-any.whl (1.0MB)\n","\u001b[K     |████████████████████████████████| 1.0MB 28.3MB/s \n","\u001b[?25hCollecting cloudpickle~=1.2.0\n","  Downloading https://files.pythonhosted.org/packages/c1/49/334e279caa3231255725c8e860fa93e72083567625573421db8875846c14/cloudpickle-1.2.2-py2.py3-none-any.whl\n","Requirement already satisfied: opencv-python in /usr/local/lib/python3.6/dist-packages (from gym==0.15.4) (4.1.2.30)\n","Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from pyglet<=1.3.2,>=1.2.0->gym==0.15.4) (0.16.0)\n","Building wheels for collected packages: gym\n","  Building wheel for gym (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for gym: filename=gym-0.15.4-cp36-none-any.whl size=1648486 sha256=e66e466849a4ee8b4c14c3cde56e2d375d64240587b349456fa5de846cdbd1fc\n","  Stored in directory: /root/.cache/pip/wheels/e9/26/9b/8a1a6599a91077a938ac4348cc3d3ac84bfab0dbfddeb4c6e7\n","Successfully built gym\n","\u001b[31mERROR: tensorflow-probability 0.11.0 has requirement cloudpickle==1.3, but you'll have cloudpickle 1.2.2 which is incompatible.\u001b[0m\n","Installing collected packages: pyglet, cloudpickle, gym\n","  Found existing installation: pyglet 1.5.0\n","    Uninstalling pyglet-1.5.0:\n","      Successfully uninstalled pyglet-1.5.0\n","  Found existing installation: cloudpickle 1.3.0\n","    Uninstalling cloudpickle-1.3.0:\n","      Successfully uninstalled cloudpickle-1.3.0\n","  Found existing installation: gym 0.17.2\n","    Uninstalling gym-0.17.2:\n","      Successfully uninstalled gym-0.17.2\n","Successfully installed cloudpickle-1.2.2 gym-0.15.4 pyglet-1.3.2\n","Collecting pygame\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/8e/24/ede6428359f913ed9cd1643dd5533aefeb5a2699cc95bea089de50ead586/pygame-1.9.6-cp36-cp36m-manylinux1_x86_64.whl (11.4MB)\n","\u001b[K     |████████████████████████████████| 11.4MB 263kB/s \n","\u001b[?25hInstalling collected packages: pygame\n","Successfully installed pygame-1.9.6\n","Reading package lists... Done\n","Building dependency tree       \n","Reading state information... Done\n","Suggested packages:\n","  libgle3\n","The following NEW packages will be installed:\n","  python-opengl\n","0 upgraded, 1 newly installed, 0 to remove and 21 not upgraded.\n","Need to get 496 kB of archives.\n","After this operation, 5,416 kB of additional disk space will be used.\n","Get:1 http://archive.ubuntu.com/ubuntu bionic/universe amd64 python-opengl all 3.1.0+dfsg-1 [496 kB]\n","Fetched 496 kB in 1s (429 kB/s)\n","Selecting previously unselected package python-opengl.\n","(Reading database ... 144611 files and directories currently installed.)\n","Preparing to unpack .../python-opengl_3.1.0+dfsg-1_all.deb ...\n","Unpacking python-opengl (3.1.0+dfsg-1) ...\n","Setting up python-opengl (3.1.0+dfsg-1) ...\n","Reading package lists... Done\n","Building dependency tree       \n","Reading state information... Done\n","The following NEW packages will be installed:\n","  xvfb\n","0 upgraded, 1 newly installed, 0 to remove and 21 not upgraded.\n","Need to get 783 kB of archives.\n","After this operation, 2,266 kB of additional disk space will be used.\n","Get:1 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 xvfb amd64 2:1.19.6-1ubuntu4.7 [783 kB]\n","Fetched 783 kB in 1s (626 kB/s)\n","Selecting previously unselected package xvfb.\n","(Reading database ... 146966 files and directories currently installed.)\n","Preparing to unpack .../xvfb_2%3a1.19.6-1ubuntu4.7_amd64.deb ...\n","Unpacking xvfb (2:1.19.6-1ubuntu4.7) ...\n","Setting up xvfb (2:1.19.6-1ubuntu4.7) ...\n","Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n","Collecting pyvirtualdisplay\n","  Downloading https://files.pythonhosted.org/packages/d0/8a/643043cc70791367bee2d19eb20e00ed1a246ac48e5dbe57bbbcc8be40a9/PyVirtualDisplay-1.3.2-py2.py3-none-any.whl\n","Collecting EasyProcess\n","  Downloading https://files.pythonhosted.org/packages/48/3c/75573613641c90c6d094059ac28adb748560d99bd27ee6f80cce398f404e/EasyProcess-0.3-py2.py3-none-any.whl\n","Installing collected packages: EasyProcess, pyvirtualdisplay\n","Successfully installed EasyProcess-0.3 pyvirtualdisplay-1.3.2\n","Collecting piglet\n","  Downloading https://files.pythonhosted.org/packages/11/56/6840e5f45626dc7eb7cd5dff57d11880b3113723b3b7b1fb1fa537855b75/piglet-1.0.0-py2.py3-none-any.whl\n","Collecting piglet-templates\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d0/dc/d628dcdf0b38b8f230e9c2309bfa370d2e3fb95e9e9c260213d10fde91ac/piglet_templates-1.0.0-py3-none-any.whl (63kB)\n","\u001b[K     |████████████████████████████████| 71kB 3.4MB/s \n","\u001b[?25hCollecting Parsley\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/2b/d6/4fed8d65e28a970e1c5cb33ce9c7e22e3de745e1b2ae37af051ef16aea3b/Parsley-1.3-py2.py3-none-any.whl (88kB)\n","\u001b[K     |████████████████████████████████| 92kB 7.3MB/s \n","\u001b[?25hRequirement already satisfied: astunparse in /usr/local/lib/python3.6/dist-packages (from piglet-templates->piglet) (1.6.3)\n","Requirement already satisfied: markupsafe in /usr/local/lib/python3.6/dist-packages (from piglet-templates->piglet) (1.1.1)\n","Requirement already satisfied: attrs in /usr/local/lib/python3.6/dist-packages (from piglet-templates->piglet) (20.2.0)\n","Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.6/dist-packages (from astunparse->piglet-templates->piglet) (0.35.1)\n","Requirement already satisfied: six<2.0,>=1.6.1 in /usr/local/lib/python3.6/dist-packages (from astunparse->piglet-templates->piglet) (1.15.0)\n","Installing collected packages: Parsley, piglet-templates, piglet\n","Successfully installed Parsley-1.3 piglet-1.0.0 piglet-templates-1.0.0\n","Reading package lists... Done\n","Building dependency tree       \n","Reading state information... Done\n","ffmpeg is already the newest version (7:3.4.8-0ubuntu0.2).\n","xvfb is already the newest version (2:1.19.6-1ubuntu4.7).\n","0 upgraded, 0 newly installed, 0 to remove and 21 not upgraded.\n","Requirement already satisfied: imageio in /usr/local/lib/python3.6/dist-packages (2.4.1)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from imageio) (1.18.5)\n","Requirement already satisfied: pillow in /usr/local/lib/python3.6/dist-packages (from imageio) (7.0.0)\n","Requirement already satisfied: PILLOW in /usr/local/lib/python3.6/dist-packages (7.0.0)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"5yTRSgI-ryd-","executionInfo":{"status":"ok","timestamp":1603647200648,"user_tz":420,"elapsed":27984,"user":{"displayName":"Yiheng Li","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ggo9FSuWqtcs815uMwgcd6czNNqCSqb_lRojxQg=s64","userId":"15227785711110309391"}},"outputId":"19b0aad2-8c68-420c-c0d7-f6f216db6917","colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["#@title Download Mujoco from an online repository\n","\n","MJC_PATH = '{}/mujoco'.format(SYM_PATH)\n","if not os.path.exists(MJC_PATH):\n","  %mkdir $MJC_PATH\n","%cd $MJC_PATH\n","if not os.path.exists(os.path.join(MJC_PATH, 'mujoco200')):\n","  !wget -q https://www.roboti.us/download/mujoco200_linux.zip\n","  !unzip -q mujoco200_linux.zip\n","  %mv mujoco200_linux mujoco200\n","  %rm mujoco200_linux.zip"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/content/gdrive/My Drive/cs330_fall2020/mujoco\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"fQJfUoanr3pm","cellView":"both"},"source":["#@title Important: ACTION Required BEFORE running this cell\n","#@markdown Place the mujoco key we have given you into a text file called mjkey.txt \n","#@markdown and ensure that the mujoco key is in the Google Drive path `cs330_fall2020/mujoco`.\n","\n","import os\n","\n","os.environ['LD_LIBRARY_PATH'] += ':{}/mujoco200/bin'.format(MJC_PATH)\n","os.environ['MUJOCO_PY_MUJOCO_PATH'] = '{}/mujoco200'.format(MJC_PATH)\n","os.environ['MUJOCO_PY_MJKEY_PATH'] = '{}/mjkey.txt'.format(MJC_PATH)\n","\n","## installation on colab does not find *.so files\n","## in LD_LIBRARY_PATH, copy over manually instead\n","!cp $MJC_PATH/mujoco200/bin/*.so /usr/lib/x86_64-linux-gnu/"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"IDYMDKI8hrHF","executionInfo":{"status":"ok","timestamp":1603647233043,"user_tz":420,"elapsed":57941,"user":{"displayName":"Yiheng Li","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ggo9FSuWqtcs815uMwgcd6czNNqCSqb_lRojxQg=s64","userId":"15227785711110309391"}},"outputId":"f7df1e8e-b968-4fa5-e9ba-b59262834627","colab":{"base_uri":"https://localhost:8080/","height":561}},"source":["#@title Important system updates for mujoco-py\n","!apt update \n","!apt install -y --no-install-recommends \\\n","        build-essential \\\n","        curl \\\n","        git \\\n","        gnupg2 \\\n","        make \\\n","        cmake \\\n","        ffmpeg \\\n","        swig \\\n","        libz-dev \\\n","        unzip \\\n","        zlib1g-dev \\\n","        libglfw3 \\\n","        libglfw3-dev \\\n","        libxrandr2 \\\n","        libxinerama-dev \\\n","        libxi6 \\\n","        libxcursor-dev \\\n","        libgl1-mesa-dev \\\n","        libgl1-mesa-glx \\\n","        libglew-dev \\\n","        libosmesa6-dev \\\n","        lsb-release \\\n","        ack-grep \\\n","        patchelf \\\n","        wget \\\n","        xpra \\\n","        xserver-xorg-dev \\\n","        xvfb \\\n","        python-opengl \\\n","        ffmpeg > /dev/null 2>&1"],"execution_count":null,"outputs":[{"output_type":"stream","text":["\u001b[33m\r0% [Working]\u001b[0m\r            \rGet:1 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran40/ InRelease [3,626 B]\n","\u001b[33m\r0% [Connecting to archive.ubuntu.com] [Connecting to security.ubuntu.com (91.18\u001b[0m\u001b[33m\r0% [Connecting to archive.ubuntu.com] [Connecting to security.ubuntu.com (91.18\u001b[0m\r                                                                               \rIgn:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  InRelease\n","\u001b[33m\r0% [Connecting to archive.ubuntu.com] [Connecting to security.ubuntu.com (91.18\u001b[0m\u001b[33m\r0% [1 InRelease gpgv 3,626 B] [Connecting to archive.ubuntu.com] [Connecting to\u001b[0m\r                                                                               \rIgn:3 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  InRelease\n","Get:4 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Release [697 B]\n","Get:5 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  Release [564 B]\n","Get:6 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Release.gpg [836 B]\n","Get:7 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  Release.gpg [833 B]\n","Get:8 http://security.ubuntu.com/ubuntu bionic-security InRelease [88.7 kB]\n","Get:9 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic InRelease [15.9 kB]\n","Get:10 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran40/ Packages [39.3 kB]\n","Hit:11 http://archive.ubuntu.com/ubuntu bionic InRelease\n","Ign:12 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Packages\n","Get:12 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Packages [370 kB]\n","Get:13 http://archive.ubuntu.com/ubuntu bionic-updates InRelease [88.7 kB]\n","Get:14 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  Packages [57.0 kB]\n","Hit:15 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic InRelease\n","Get:16 http://security.ubuntu.com/ubuntu bionic-security/restricted amd64 Packages [211 kB]\n","Get:17 http://archive.ubuntu.com/ubuntu bionic-backports InRelease [74.6 kB]\n","Get:18 http://security.ubuntu.com/ubuntu bionic-security/main amd64 Packages [1,745 kB]\n","Get:19 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic/main Sources [1,685 kB]\n","Get:20 http://security.ubuntu.com/ubuntu bionic-security/multiverse amd64 Packages [15.4 kB]\n","Get:21 http://security.ubuntu.com/ubuntu bionic-security/universe amd64 Packages [1,352 kB]\n","Get:22 http://archive.ubuntu.com/ubuntu bionic-updates/multiverse amd64 Packages [45.9 kB]\n","Get:23 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 Packages [2,162 kB]\n","Get:24 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic/main amd64 Packages [863 kB]\n","Get:25 http://archive.ubuntu.com/ubuntu bionic-updates/restricted amd64 Packages [238 kB]\n","Get:26 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 Packages [2,115 kB]\n","Fetched 11.2 MB in 3s (3,251 kB/s)\n","Reading package lists... Done\n","Building dependency tree       \n","Reading state information... Done\n","51 packages can be upgraded. Run 'apt list --upgradable' to see them.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"tcG4cdIysCu_","executionInfo":{"status":"ok","timestamp":1603647256368,"user_tz":420,"elapsed":79561,"user":{"displayName":"Yiheng Li","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ggo9FSuWqtcs815uMwgcd6czNNqCSqb_lRojxQg=s64","userId":"15227785711110309391"}},"outputId":"fde71fa4-573b-4482-c7db-c83abe15c7cb","colab":{"base_uri":"https://localhost:8080/","height":598}},"source":["#@title Clone and install mujoco-py\n","#@markdown Remember that you need to put the key in the appropriate location as described above\n","%cd $MJC_PATH\n","if not os.path.exists('mujoco-py'):\n","  !git clone https://github.com/openai/mujoco-py.git\n","%cd mujoco-py\n","%pip install -e .\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/content/gdrive/My Drive/cs330_fall2020/mujoco\n","Cloning into 'mujoco-py'...\n","remote: Enumerating objects: 97, done.\u001b[K\n","remote: Counting objects: 100% (97/97), done.\u001b[K\n","remote: Compressing objects: 100% (67/67), done.\u001b[K\n","remote: Total 2148 (delta 69), reused 55 (delta 30), pack-reused 2051\u001b[K\n","Receiving objects: 100% (2148/2148), 5.62 MiB | 11.91 MiB/s, done.\n","Resolving deltas: 100% (1335/1335), done.\n","Checking out files: 100% (200/200), done.\n","/content/gdrive/My Drive/cs330_fall2020/mujoco/mujoco-py\n","Obtaining file:///content/gdrive/My%20Drive/cs330_fall2020/mujoco/mujoco-py\n","  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n","\u001b[33m  WARNING: Missing build requirements in pyproject.toml for file:///content/gdrive/My%20Drive/cs330_fall2020/mujoco/mujoco-py.\u001b[0m\n","\u001b[33m  WARNING: The project does not specify a build backend, and pip cannot fall back to setuptools without 'wheel'.\u001b[0m\n","  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n","  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n","    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: numpy>=1.11 in /usr/local/lib/python3.6/dist-packages (from mujoco-py==2.0.2.13) (1.18.5)\n","Collecting glfw>=1.4.0\n","  Using cached https://files.pythonhosted.org/packages/12/72/3190b7cc8494c05d7fb350237d3f51abdaff79e74d1e13d6f84df4b57f6b/glfw-2.0.0-py2.py27.py3.py30.py31.py32.py33.py34.py35.py36.py37.py38-none-manylinux2014_x86_64.whl\n","Requirement already satisfied: imageio>=2.1.2 in /usr/local/lib/python3.6/dist-packages (from mujoco-py==2.0.2.13) (2.4.1)\n","Requirement already satisfied: Cython>=0.27.2 in /usr/local/lib/python3.6/dist-packages (from mujoco-py==2.0.2.13) (0.29.21)\n","Collecting fasteners~=0.15\n","  Using cached https://files.pythonhosted.org/packages/18/bd/55eb2d6397b9c0e263af9d091ebdb756b15756029b3cededf6461481bc63/fasteners-0.15-py2.py3-none-any.whl\n","Requirement already satisfied: cffi>=1.10 in /usr/local/lib/python3.6/dist-packages (from mujoco-py==2.0.2.13) (1.14.3)\n","Requirement already satisfied: pillow in /usr/local/lib/python3.6/dist-packages (from imageio>=2.1.2->mujoco-py==2.0.2.13) (7.0.0)\n","Collecting monotonic>=0.1\n","  Using cached https://files.pythonhosted.org/packages/ac/aa/063eca6a416f397bd99552c534c6d11d57f58f2e94c14780f3bbf818c4cf/monotonic-1.5-py2.py3-none-any.whl\n","Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from fasteners~=0.15->mujoco-py==2.0.2.13) (1.15.0)\n","Requirement already satisfied: pycparser in /usr/local/lib/python3.6/dist-packages (from cffi>=1.10->mujoco-py==2.0.2.13) (2.20)\n","Installing collected packages: glfw, monotonic, fasteners, mujoco-py\n","  Running setup.py develop for mujoco-py\n","Successfully installed fasteners-0.15 glfw-2.0.0 monotonic-1.5 mujoco-py\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"ba-tk-JcOsgM","executionInfo":{"status":"ok","timestamp":1603647312936,"user_tz":420,"elapsed":134813,"user":{"displayName":"Yiheng Li","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ggo9FSuWqtcs815uMwgcd6czNNqCSqb_lRojxQg=s64","userId":"15227785711110309391"}},"outputId":"8fc57afd-5695-49c9-81c6-81abcc83d991","colab":{"base_uri":"https://localhost:8080/","height":360}},"source":["## cythonize at the first import\n","import mujoco_py"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Compiling /content/gdrive/My Drive/cs330_fall2020/mujoco/mujoco-py/mujoco_py/cymj.pyx because it changed.\n","[1/1] Cythonizing /content/gdrive/My Drive/cs330_fall2020/mujoco/mujoco-py/mujoco_py/cymj.pyx\n","running build_ext\n","building 'mujoco_py.cymj' extension\n","creating /content/gdrive/My Drive/cs330_fall2020/mujoco/mujoco-py/mujoco_py/generated/_pyxbld_2.0.2.13_36_linuxcpuextensionbuilder\n","creating /content/gdrive/My Drive/cs330_fall2020/mujoco/mujoco-py/mujoco_py/generated/_pyxbld_2.0.2.13_36_linuxcpuextensionbuilder/temp.linux-x86_64-3.6\n","creating /content/gdrive/My Drive/cs330_fall2020/mujoco/mujoco-py/mujoco_py/generated/_pyxbld_2.0.2.13_36_linuxcpuextensionbuilder/temp.linux-x86_64-3.6/content\n","creating /content/gdrive/My Drive/cs330_fall2020/mujoco/mujoco-py/mujoco_py/generated/_pyxbld_2.0.2.13_36_linuxcpuextensionbuilder/temp.linux-x86_64-3.6/content/gdrive\n","creating /content/gdrive/My Drive/cs330_fall2020/mujoco/mujoco-py/mujoco_py/generated/_pyxbld_2.0.2.13_36_linuxcpuextensionbuilder/temp.linux-x86_64-3.6/content/gdrive/My Drive\n","creating /content/gdrive/My Drive/cs330_fall2020/mujoco/mujoco-py/mujoco_py/generated/_pyxbld_2.0.2.13_36_linuxcpuextensionbuilder/temp.linux-x86_64-3.6/content/gdrive/My Drive/cs330_fall2020\n","creating /content/gdrive/My Drive/cs330_fall2020/mujoco/mujoco-py/mujoco_py/generated/_pyxbld_2.0.2.13_36_linuxcpuextensionbuilder/temp.linux-x86_64-3.6/content/gdrive/My Drive/cs330_fall2020/mujoco\n","creating /content/gdrive/My Drive/cs330_fall2020/mujoco/mujoco-py/mujoco_py/generated/_pyxbld_2.0.2.13_36_linuxcpuextensionbuilder/temp.linux-x86_64-3.6/content/gdrive/My Drive/cs330_fall2020/mujoco/mujoco-py\n","creating /content/gdrive/My Drive/cs330_fall2020/mujoco/mujoco-py/mujoco_py/generated/_pyxbld_2.0.2.13_36_linuxcpuextensionbuilder/temp.linux-x86_64-3.6/content/gdrive/My Drive/cs330_fall2020/mujoco/mujoco-py/mujoco_py\n","creating /content/gdrive/My Drive/cs330_fall2020/mujoco/mujoco-py/mujoco_py/generated/_pyxbld_2.0.2.13_36_linuxcpuextensionbuilder/temp.linux-x86_64-3.6/content/gdrive/My Drive/cs330_fall2020/mujoco/mujoco-py/mujoco_py/gl\n","x86_64-linux-gnu-gcc -pthread -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -Imujoco_py -I/content/gdrive/My Drive/cs330_fall2020/mujoco/mujoco-py/mujoco_py -I/content/cs330_fall2020/mujoco/mujoco200/include -I/usr/local/lib/python3.6/dist-packages/numpy/core/include -I/usr/include/python3.6m -c /content/gdrive/My Drive/cs330_fall2020/mujoco/mujoco-py/mujoco_py/cymj.c -o /content/gdrive/My Drive/cs330_fall2020/mujoco/mujoco-py/mujoco_py/generated/_pyxbld_2.0.2.13_36_linuxcpuextensionbuilder/temp.linux-x86_64-3.6/content/gdrive/My Drive/cs330_fall2020/mujoco/mujoco-py/mujoco_py/cymj.o -fopenmp -w\n","x86_64-linux-gnu-gcc -pthread -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -Imujoco_py -I/content/gdrive/My Drive/cs330_fall2020/mujoco/mujoco-py/mujoco_py -I/content/cs330_fall2020/mujoco/mujoco200/include -I/usr/local/lib/python3.6/dist-packages/numpy/core/include -I/usr/include/python3.6m -c /content/gdrive/My Drive/cs330_fall2020/mujoco/mujoco-py/mujoco_py/gl/osmesashim.c -o /content/gdrive/My Drive/cs330_fall2020/mujoco/mujoco-py/mujoco_py/generated/_pyxbld_2.0.2.13_36_linuxcpuextensionbuilder/temp.linux-x86_64-3.6/content/gdrive/My Drive/cs330_fall2020/mujoco/mujoco-py/mujoco_py/gl/osmesashim.o -fopenmp -w\n","creating /content/gdrive/My Drive/cs330_fall2020/mujoco/mujoco-py/mujoco_py/generated/_pyxbld_2.0.2.13_36_linuxcpuextensionbuilder/lib.linux-x86_64-3.6\n","creating /content/gdrive/My Drive/cs330_fall2020/mujoco/mujoco-py/mujoco_py/generated/_pyxbld_2.0.2.13_36_linuxcpuextensionbuilder/lib.linux-x86_64-3.6/mujoco_py\n","x86_64-linux-gnu-gcc -pthread -shared -Wl,-O1 -Wl,-Bsymbolic-functions -Wl,-Bsymbolic-functions -Wl,-z,relro -Wl,-Bsymbolic-functions -Wl,-z,relro -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 /content/gdrive/My Drive/cs330_fall2020/mujoco/mujoco-py/mujoco_py/generated/_pyxbld_2.0.2.13_36_linuxcpuextensionbuilder/temp.linux-x86_64-3.6/content/gdrive/My Drive/cs330_fall2020/mujoco/mujoco-py/mujoco_py/cymj.o /content/gdrive/My Drive/cs330_fall2020/mujoco/mujoco-py/mujoco_py/generated/_pyxbld_2.0.2.13_36_linuxcpuextensionbuilder/temp.linux-x86_64-3.6/content/gdrive/My Drive/cs330_fall2020/mujoco/mujoco-py/mujoco_py/gl/osmesashim.o -L/content/cs330_fall2020/mujoco/mujoco200/bin -Wl,--enable-new-dtags,-R/content/cs330_fall2020/mujoco/mujoco200/bin -lmujoco200 -lglewosmesa -lOSMesa -lGL -o /content/gdrive/My Drive/cs330_fall2020/mujoco/mujoco-py/mujoco_py/generated/_pyxbld_2.0.2.13_36_linuxcpuextensionbuilder/lib.linux-x86_64-3.6/mujoco_py/cymj.cpython-36m-x86_64-linux-gnu.so -fopenmp\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"AGGFqjdcsX3g","executionInfo":{"status":"ok","timestamp":1603647342351,"user_tz":420,"elapsed":17384,"user":{"displayName":"Yiheng Li","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ggo9FSuWqtcs815uMwgcd6czNNqCSqb_lRojxQg=s64","userId":"15227785711110309391"}},"outputId":"db90ff69-7867-40ec-f567-044f5f92d022","colab":{"base_uri":"https://localhost:8080/","height":136}},"source":["#@title Clone and install multiworld\n","%cd $SYM_PATH\n","!git clone https://github.com/vitchyr/multiworld.git\n","\n","%cd multiworld\n","%pip install -e .\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/content/gdrive/My Drive/cs330_fall2020\n","fatal: destination path 'multiworld' already exists and is not an empty directory.\n","/content/gdrive/My Drive/cs330_fall2020/multiworld\n","Obtaining file:///content/gdrive/My%20Drive/cs330_fall2020/multiworld\n","Installing collected packages: multiworld\n","  Running setup.py develop for multiworld\n","Successfully installed multiworld\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"0q6Swy46pzyg","executionInfo":{"status":"ok","timestamp":1603647345245,"user_tz":420,"elapsed":658,"user":{"displayName":"Yiheng Li","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ggo9FSuWqtcs815uMwgcd6czNNqCSqb_lRojxQg=s64","userId":"15227785711110309391"}},"outputId":"651e9e74-f3bf-4146-9da5-ccde8cef1fea","colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["#@title Sets up virtual display\n","from pyvirtualdisplay import Display\n","display = Display(visible=0, size=(1400, 900))\n","display.start()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<pyvirtualdisplay.display.Display at 0x7f35eb57c978>"]},"metadata":{"tags":[]},"execution_count":9}]},{"cell_type":"code","metadata":{"id":"2Zc9_-pRitwk"},"source":["#@title Check imports and add helper functions for display\n","\n","import os\n","import gym\n","from gym import logger as gymlogger\n","from gym.wrappers import Monitor\n","gymlogger.set_level(40) # error only\n","import tensorflow as tf\n","import numpy as np\n","import random\n","import matplotlib\n","import matplotlib.pyplot as plt\n","import math\n","import glob\n","import io\n","import base64\n","from IPython.display import HTML\n","\n","from IPython import display as ipythondisplay\n","if type(os.environ.get(\"DISPLAY\")) is not str or len(os.environ.get(\"DISPLAY\"))==0:\n","    !bash ../xvfb start\n","    %env DISPLAY=:1\n","\n","def show_video():\n","  mp4list = glob.glob('video/*.mp4')\n","  if len(mp4list) > 0:\n","    mp4 = mp4list[0]\n","    video = io.open(mp4, 'r+b').read()\n","    encoded = base64.b64encode(video)\n","    ipythondisplay.display(HTML(data='''<video alt=\"test\" autoplay \n","                loop controls style=\"height: 400px;\">\n","                <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\" />\n","             </video>'''.format(encoded.decode('ascii'))))\n","  else: \n","    print(\"Could not find video\")\n","    \n","\n","def wrap_env(env):\n","  env = Monitor(env, './video', force=True)\n","  return env"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"c3uYa0KHs0LB","executionInfo":{"status":"ok","timestamp":1603647351764,"user_tz":420,"elapsed":2737,"user":{"displayName":"Yiheng Li","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ggo9FSuWqtcs815uMwgcd6czNNqCSqb_lRojxQg=s64","userId":"15227785711110309391"}},"outputId":"81168557-90ce-486f-9202-b960aa8d2ad1","colab":{"base_uri":"https://localhost:8080/","height":438}},"source":["#@title After running, you should see a video play\n","matplotlib.use('Agg')\n","\n","env = wrap_env(gym.make(\"Ant-v2\"))\n","\n","observation = env.reset()\n","for i in range(10):\n","    env.render(mode='rgb_array')\n","    obs, rew, term, _ = env.step(env.action_space.sample() ) \n","    if term:\n","      break;\n","            \n","env.close()\n","print('Loading video...')\n","show_video()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Loading video...\n"],"name":"stdout"},{"output_type":"display_data","data":{"text/html":["<video alt=\"test\" autoplay \n","                loop controls style=\"height: 400px;\">\n","                <source src=\"data:video/mp4;base64,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\" type=\"video/mp4\" />\n","             </video>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}}]},{"cell_type":"code","metadata":{"id":"v6uncjbtGb_s"},"source":["#@title Random seed is set to be fixed\n","import tensorflow\n","tensorflow.random.set_seed(330)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-mD5hrg3cPn7"},"source":["# BitFlip Goal Conditioned RL\n","\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"PpGZxJIg9ghR"},"source":["## BitFlipEnv\n","\n","Familiarize yourself with what the bit flip environment does and what each method does.\n","\n","You do *NOT* need to modify the following cell."]},{"cell_type":"code","metadata":{"id":"kKw1cs-ZiAjt"},"source":["class BitFlipEnv():\n","    '''bit flipping environment for reinforcement learning.\n","    The environment is a 1D vector of binary values (state vector).\n","    At each step, the actor can flip a single bit (0 to 1 or 1 to 0).\n","    The goal is to flip bits until the state vector matches the\n","    goal vector (also a 1D vector of binary values). At each step,\n","    the actor receives a goal of 0 if the state and goal vector\n","    do not match and a reward of 1 if the state and goal vector\n","    match.\n","\n","    Internally the state and goal vector are a numpy array, which\n","    allows the vectors to be printed by the show_goal and show_state\n","    methods. When '''\n","\n","    def __init__(self, num_bits, verbose = False):\n","        '''Initialize new instance of BitFlip class.\n","        inputs: num_bits - number of bits in the environment; must\n","                be an integer\n","                verbose - prints state and goal vector after each\n","                          step if True'''\n","\n","        # check that num_bits is a positive integer\n","        if (num_bits < 0) or (type(num_bits) != type(0)):\n","            print(\"Invalid number of bits -  must be positive integer\")\n","            return\n","\n","        # number of bits in the environment\n","        self.num_bits = num_bits\n","        # randomly set the state vector\n","        self.state_vector = np.random.randint(0, 2, num_bits)\n","        # randomly set the goal vector\n","        self.goal_vector = np.random.randint(0, 2, num_bits)\n","        # whether to print debugging info\n","        self.verbose = verbose\n","        # TODO set dimensions of observation space\n","        self.observation_space = self.state_vector\n","        # TODO create action space; may use gym type\n","        self.action_space = num_bits\n","        # space of the goal vector\n","        self.goal_space = self.goal_vector\n","        # number of steps taken\n","        self.steps = 0\n","\n","        return\n","\n","    def show_goal(self):\n","        '''Returns the goal as a numpy array. Used for debugging.'''\n","        return self.goal_vector\n","\n","    def show_state(self):\n","        '''Returns the state as a numpy array. Used for debugging.'''\n","        return self.state_vector\n","\n","    def reset(self):\n","        '''resets the environment. Returns a reset state_vector\n","        and goal_vector as tf tensors'''\n","\n","        # randomly reset both the state and the goal vectors\n","        self.state_vector = np.random.randint(0, 2, self.num_bits)\n","        self.goal_vector = np.random.randint(0, 2, self.num_bits)\n","        self.steps = 0\n","\n","        # return as np array\n","        return self.state_vector, self.goal_vector\n","\n","\n","    def step(self, action):\n","        '''take a step and flip one of the bits.\n","\n","        inputs: action - integer index of the bit to flip\n","        outputs: state - new state_vector (tensor)\n","                 reward - 0 if state != goal and 1 if state == goal\n","                 done - boolean value indicating if the goal has been reached'''\n","        self.steps += 1\n","\n","\n","        if action < 0 or action >= self.num_bits:\n","            # check argument is in range\n","            print(\"Invalid action! Must be integer ranging from \\\n","                0 to num_bits-1\")\n","            return\n","\n","        # flip the bit with index action\n","        if self.state_vector[action] == 1:\n","            self.state_vector[action] = 0\n","        else:\n","            self.state_vector[action] = 1\n","\n","        # initial values of reward and done - may change\n","        # depending on state and goal vectors\n","        reward = 0\n","        done = True\n","\n","        # check if state and goal vectors are identical\n","        if False in (self.state_vector == self.goal_vector):\n","            reward = -1\n","            done = False\n","\n","        # print additional info if verbose mode is on\n","        if self.verbose:\n","            print(\"Bit flipped:   \", action)\n","            print(\"Goal vector:   \", self.goal_vector)\n","            print(\"Updated state: \", self.state_vector)\n","            print(\"Reward:        \", reward)\n","\n","        if done:\n","            #print(\"Solved in: \", self.steps)\n","            pass\n","\n","        # return state as numpy arrays\n","        # return goal_vector in info field\n","        return np.copy(self.state_vector), reward, done, self.steps\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"GZOi6L7Pdgd0"},"source":["## Buffer\n","Familiarize yourself with what the buffer does \n","\n","You do *NOT* need to modify the following cell."]},{"cell_type":"code","metadata":{"id":"J14LUXqwkC9e"},"source":["import numpy as np\n","import random\n","from collections import deque \n","\n","class Buffer(object) :\n","\n","\tdef __init__(self,size,sample_size):\n","\n","\t\tself.size = size\n","\t\tself.sample_size = sample_size\n","\t\tself.buffer = deque()\n","\n","\tdef add(self,state,action,reward,next_state) :\n","\t\tself.buffer.append((state,action,reward,next_state))\n","\n","\t\tif len(self.buffer) > self.size:\n","\t\t\tself.buffer.popleft()\n","\n","\tdef sample(self) :\n","\t\tif len(self.buffer) < self.sample_size:\n","\t\t\tsamples = self.buffer\n","\t\telse:\t\n","\t\t\tsamples = random.sample(self.buffer,self.sample_size)\n","\t\t\n","\t\tstate = np.reshape(np.array([arr[0] for arr in samples]),[len(samples),-1])\n","\t\taction = np.array([arr[1] for arr in samples])\n","\t\treward = np.array([arr[2] for arr in samples])\n","\t\tnext_state = np.reshape(np.array([arr[3] for arr in samples]),[len(samples),-1])\n","\n","\t\treturn state, action, reward, next_state\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"JPtYOkhghoz-"},"source":["## BitFlip Goal Condition RL and Training\n","\n","Implement the changes you need for Problems 1-3 here in the cells below."]},{"cell_type":"code","metadata":{"id":"jgoB2zVqi2G8"},"source":["import numpy as np\n","import tensorflow as tf\n","from matplotlib import pyplot as plt\n","\n","\n","class Model(tf.keras.Model):\n","\n","  def __init__(self, num_bits):\n","    super(Model, self).__init__()\n","\n","    hidden_dim = 256\n","    self.dense1 = tf.keras.layers.Dense(hidden_dim, activation=tf.nn.relu)\n","    self.out = tf.keras.layers.Dense(num_bits,activation = None)\n","\n","  def call(self, inputs):\n","\n","    x = self.dense1(inputs)\n","    return self.out(x)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"rXD2QYkHh56I"},"source":["# ************   Helper functions    ************ #\n","\n","def updateTarget(model, target_model, tau=0.95) :\n","    model_weights = model.get_weights()\n","    target_weights = target_model.get_weights()\n","    new_weights = []\n","    for i, weight in enumerate(model_weights):\n","      new_weights.append(tau * target_weights[i] + (1 - tau) * weight)\n","\n","    target_model.set_weights(new_weights)\n","\n","def solve_environment(num_bits, model, bit_env, state, goal_state, total_reward):\n","    '''attempt to solve the bit flipping environment using the current policy\n","\n","    inputs: num_bits - number of bits to be looped over \n","        model - DQN to run prediction on\n","        bit_env - environment for bitflip\n","        state - current state\n","        goal_state - desired state\n","        total_reward - cumulative reward so far\n","    '''\n","    \n","    # list for recording what happened in the episode\n","    episode_experience = []\n","    succeeded = False\n","\n","    for t in range(num_bits):\n","      \n","      # attempt to solve the state - number of steps given to solve the\n","      # state is equal to the size of the vector\n","      \n","      # ======================== TODO modify code ========================\n","      # (concatnate state and goal)\n","      inputs = np.concatenate([state, goal_state])\n","      inputs = np.expand_dims(inputs, axis=0)\n","      # forward pass to find action, (concatnate state and goal)\n","      out = model(inputs)\n","      action = np.argmax(out,axis=1)\n","      # take the action\n","      next_state,reward,done, _ = bit_env.step(action)\n","      # add to the episode experience (what happened)\n","      episode_experience.append((state,action,reward,next_state,goal_state))\n","      # calculate total reward\n","      total_reward+=reward\n","\n","      # update state\n","      state = next_state\n","      # mark that we've finished the episode and succeeded with training\n","      if done:\n","        if succeeded:\n","            continue\n","        else:\n","            succeeded = True\n","\n","      # ========================      END TODO       ========================\n","\n","\n","    return succeeded, episode_experience, total_reward\n","\n","def solve_environment_no_goal(num_bits, model, bit_env, state, goal_state, total_reward):\n","    '''attempt to solve the bit_flip env using no goal'''\n","    \n","    # list for recording what happened in the episode\n","    episode_experience = []\n","    succeeded = False\n","\n","    for t in range(num_bits):\n","        # attempt to solve the state - number of steps given to solve the\n","        # state is equal to the passed argument steps_per_episode.\n","\n","        inputs = state\n","        inputs = np.expand_dims(inputs, axis=0)\n","        # forward pass to find action\n","        action = np.argmax(out,axis = 1)\n","        # take the action\n","        next_state,reward,done, _ = bit_env.step(action)\n","        # add to the episode experience (what happened)\n","        episode_experience.append((state,action,reward,next_state,goal_state))\n","        # calculate total reward\n","        total_reward+=reward\n","        # update state\n","        state = next_state\n","        # mark that we've finished the episode and succeeded with training\n","        if done:\n","            if succeeded:\n","                continue\n","            else:\n","                succeeded = True\n","\n","\n","\n","    return succeeded, episode_experi1ence, total_reward\n","\n","\n","def update_replay_buffer(num_bits, num_relabeled, replay_buffer, episode_experience, HER):\n","    '''adds past experience to the replay buffer. Training is done with episodes from the replay\n","    buffer. When HER is used, relabeled experiences are also added to the replay buffer\n","\n","    inputs: num_bits - number of bits to be looped over \n","            replay_buffer - the buffer to store past experience in\n","            episode_experience - list of transitions from the last episode\n","            HER -  type of hindsight experience replay to use\n","    modifies: replay_buffer\n","    outputs: None'''\n","\n","    _,_,_,final_state,_ = episode_experience[num_bits - 1]\n","    for t in range(num_bits) :\n","        # copy actual experience from episode_experience to replay_buffer\n","\n","        # ======================== TODO modify code ========================\n","        s,a,r,s_,g = episode_experience[t]\n","        # state\n","        inputs = np.concatenate([s,g])\n","        # next state\n","        inputs_ = np.concatenate([s_,g])\n","        # add to the replay buffer\n","        replay_buffer.add(inputs,a,r,inputs_)\n","\n","        # when HER is used, each call to update_replay_buffer should add k\n","        # relabeled point(s) to the replay buffer per bit, where k=1 if\n","        # HER==\"final\" and k=num_related if HER==\"random\" or HER==\"future\"\n","\n","        if HER == 'None':\n","            # HER not being used, so do nothing\n","            pass\n","\n","        elif HER == 'final':\n","            # final - relabel based on final state in episode\n","            if t != num_bits - 1:\n","              replay_buffer.add(np.concatenate([s,final_state]),a,r,np.concatenate([s_,final_state]))\n","            else:\n","              replay_buffer.add(np.concatenate([s,final_state]),a,0,np.concatenate([s_,final_state]))\n","\n","\n","        elif HER == 'future':\n","            # future - relabel based on future state. At each timestep t, relabel the\n","            # goal with a randomly select timestep between t and the end of the\n","            # episode\n","            for i in range(num_relabeled):\n","              future_time = random.randint(t, num_bits-1)\n","              _,_,_,future_state,_ = episode_experience[future_time]\n","              if t != future_time:\n","                replay_buffer.add(np.concatenate([s, future_state]),a,r,np.concatenate([s_,future_state]))\n","              else:\n","                replay_buffer.add(np.concatenate([s, future_state]),a,0,np.concatenate([s_,future_state]))\n","\n","\n","        elif HER == 'random':\n","            # random - relabel based on a random state in the episode\n","            for i in range(num_relabeled):\n","              random_time = random.randint(0, num_bits-1)\n","              _,_,_,random_state,_ = episode_experience[random_time]\n","              if t != random_time:\n","                replay_buffer.add(np.concatenate([s, random_state]),a,r,np.concatenate([s_,random_state]))\n","              else:\n","                replay_buffer.add(np.concatenate([s, random_state]),a,0,np.concatenate([s_,random_state]))\n","\n","        # ========================      END TODO       ========================\n","\n","\n","        else:\n","            print(\"Invalid value for Her flag - HER not used\")\n","    return\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"WL4Cedzih-eg"},"source":["\n","# ************   Main training loop    ************ #\n","\n","\n","def flip_bits(num_bits, num_epochs, buffer_size = 1e6, batch_size = 128, \n","              num_episodes = 16, num_relabeled = 4, gamma = 0.98, log_interval=5, opt_steps=40, HER = \"None\"):\n","    '''Main loop for running in the bit flipping environment. The DQN is\n","    trained over num_epochs. In each epoch, the agent runs in the environment\n","    num_episodes number of times. The Q-target and Q-policy networks are\n","    updated at the end of each epoch. Within one episode, Q-policy attempts\n","    to solve the environment and is limited to the same number as steps as the\n","    size of the environment\n","\n","    inputs: HER - string specifying whether to use HER'''\n","\n","    print(\"Running bit flip environment with %d bits and HER policy: %s\" %(num_bits, HER))\n","\n","    # create bit flipping environment and replay buffer\n","    bit_env = BitFlipEnv(num_bits)\n","    replay_buffer = Buffer(buffer_size,batch_size)\n","\n","    # set up Q-policy (model) and Q-target (target_model)\n","    model = Model(num_bits)\n","    optimizer = tf.keras.optimizers.Adam(learning_rate = 1e-3)\n","    target_model = Model(num_bits)\n","\n","    # ======================== TODO modify code ========================\n","    # modify to be goal conditioned\n","    state, goal_state = bit_env.reset()      \n","    inputs = np.concatenate([state, goal_state])\n","    \n","    inputs = np.expand_dims(inputs, axis=0)  \n","    model(inputs)\n","    target_model(inputs)\n","\n","    # start by making Q-target and Q-policy the same\n","    updateTarget(model, target_model, tau=0.0)\n","    # ========================      END TODO       ========================\n","\n","\n","    total_loss = []                  # training loss for each epoch\n","    success_rate = []                # success rate for each epoch\n","    \n","    for i in range(num_epochs):\n","        # Run for a fixed number of epochs\n","\n","        total_reward = 0.0           # total reward for the epoch\n","        successes = []               # record success rate for each episode of the epoch\n","        losses = []                  # loss at the end of each epoch\n","\n","        for k in range(num_episodes):\n","            # Run in the environment for num_episodes  \n","            state, goal_state = bit_env.reset()             # reset the environment     \n","            # attempt to solve the environment\n","            # ======================== TODO modify code ========================\n","            # modify to be goal conditioned\n","            succeeded, episode_experience, total_reward = solve_environment(num_bits, model, bit_env, state, goal_state, total_reward)\n","            # ========================     END TODO     ========================\n","            successes.append(succeeded)                     # track whether we succeeded in environment \n","            update_replay_buffer(num_bits, num_relabeled, replay_buffer, episode_experience, HER)   # add to the replay buffer; use specified  HER policy\n","        for k in range(opt_steps):\n","            # optimize the Q-policy network\n","\n","            # sample from the replay buffer\n","            state,action,reward,next_state = replay_buffer.sample()\n","            # forward pass through target network   \n","            # target_net_Q = sess.run(target_model.out,feed_dict = {target_model.inp : next_state})\n","            with tf.GradientTape() as tape:\n","              target_net_Q = target_model(next_state)\n","              # calculate target reward\n","              target_reward = np.clip(np.reshape(reward,[-1]) + gamma * np.reshape(np.max(target_net_Q,axis = -1),[-1]),-1. / (1 - gamma), 0)\n","              # calculate predictions and loss\n","              model_predict = model(state)\n","              model_action_taken = np.reshape(action,[-1])\n","              action_one_hot = tf.one_hot(model_action_taken, num_bits)\n","              Q_val = tf.reduce_sum(model_predict * action_one_hot, axis=1)\n","              loss = tf.reduce_mean(tf.square(Q_val - target_reward))\n","              losses.append(loss)\n","            \n","            gradients = tape.gradient(loss, model.trainable_variables)\n","            optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n","            \n","        updateTarget(model, target_model)               # update target model by copying Q-policy to Q-target      \n","        success_rate.append(np.mean(successes))       # append mean success rate for this epoch\n","\n","        if i % log_interval == 0:\n","            print('Epoch: %d  Cumulative reward: %f  Success rate: %.4f Mean loss: %.4f' % (i, total_reward, np.mean(successes), np.mean(losses)))\n","                \n","    return success_rate"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"uaqw28hxi6Zi","executionInfo":{"status":"ok","timestamp":1603600820293,"user_tz":420,"elapsed":45029,"user":{"displayName":"Yiheng Li","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ggo9FSuWqtcs815uMwgcd6czNNqCSqb_lRojxQg=s64","userId":"15227785711110309391"}},"outputId":"853352df-7a61-4db8-88a3-c9979a09458b","colab":{"base_uri":"https://localhost:8080/","height":544}},"source":["# Sample commands have been provided to you below\n","# run with type of HER specified\n","success_rate  = flip_bits(num_bits=7, num_epochs=150, HER='None') \n","# pass success rate for each run as first argument and labels as second list"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Running bit flip environment with 7 bits and HER policy: None\n","Epoch: 0  Cumulative reward: -112.000000  Success rate: 0.0000 Mean loss: 0.1380\n","Epoch: 5  Cumulative reward: -111.000000  Success rate: 0.0625 Mean loss: 0.0089\n","Epoch: 10  Cumulative reward: -112.000000  Success rate: 0.0000 Mean loss: 0.0066\n","Epoch: 15  Cumulative reward: -112.000000  Success rate: 0.0000 Mean loss: 0.0069\n","Epoch: 20  Cumulative reward: -112.000000  Success rate: 0.0000 Mean loss: 0.0072\n","Epoch: 25  Cumulative reward: -112.000000  Success rate: 0.0000 Mean loss: 0.0075\n","Epoch: 30  Cumulative reward: -110.000000  Success rate: 0.0625 Mean loss: 0.0070\n","Epoch: 35  Cumulative reward: -112.000000  Success rate: 0.0000 Mean loss: 0.0072\n","Epoch: 40  Cumulative reward: -112.000000  Success rate: 0.0000 Mean loss: 0.0072\n","Epoch: 45  Cumulative reward: -109.000000  Success rate: 0.0625 Mean loss: 0.0080\n","Epoch: 50  Cumulative reward: -109.000000  Success rate: 0.0625 Mean loss: 0.0092\n","Epoch: 55  Cumulative reward: -106.000000  Success rate: 0.1250 Mean loss: 0.0093\n","Epoch: 60  Cumulative reward: -112.000000  Success rate: 0.0000 Mean loss: 0.0090\n","Epoch: 65  Cumulative reward: -112.000000  Success rate: 0.0000 Mean loss: 0.0083\n","Epoch: 70  Cumulative reward: -112.000000  Success rate: 0.0000 Mean loss: 0.0094\n","Epoch: 75  Cumulative reward: -105.000000  Success rate: 0.1250 Mean loss: 0.0118\n","Epoch: 80  Cumulative reward: -112.000000  Success rate: 0.0000 Mean loss: 0.0105\n","Epoch: 85  Cumulative reward: -112.000000  Success rate: 0.0000 Mean loss: 0.0101\n","Epoch: 90  Cumulative reward: -109.000000  Success rate: 0.0625 Mean loss: 0.0105\n","Epoch: 95  Cumulative reward: -101.000000  Success rate: 0.2500 Mean loss: 0.0129\n","Epoch: 100  Cumulative reward: -111.000000  Success rate: 0.0625 Mean loss: 0.0138\n","Epoch: 105  Cumulative reward: -106.000000  Success rate: 0.1250 Mean loss: 0.0151\n","Epoch: 110  Cumulative reward: -112.000000  Success rate: 0.0000 Mean loss: 0.0159\n","Epoch: 115  Cumulative reward: -107.000000  Success rate: 0.1250 Mean loss: 0.0164\n","Epoch: 120  Cumulative reward: -98.000000  Success rate: 0.3125 Mean loss: 0.0164\n","Epoch: 125  Cumulative reward: -102.000000  Success rate: 0.1875 Mean loss: 0.0192\n","Epoch: 130  Cumulative reward: -98.000000  Success rate: 0.3125 Mean loss: 0.0220\n","Epoch: 135  Cumulative reward: -93.000000  Success rate: 0.4375 Mean loss: 0.0238\n","Epoch: 140  Cumulative reward: -99.000000  Success rate: 0.3125 Mean loss: 0.0254\n","Epoch: 145  Cumulative reward: -92.000000  Success rate: 0.4375 Mean loss: 0.0271\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"6NM7sfGBTjou"},"source":["import matplotlib.pyplot as plt\n","plt.plot(list(range(len(success_rate))), success_rate)\n","plt.savefig('p1.png', bbox_inches='tight')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"gxnKFyyUUd5b","executionInfo":{"status":"ok","timestamp":1603600140672,"user_tz":420,"elapsed":125164,"user":{"displayName":"Yiheng Li","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ggo9FSuWqtcs815uMwgcd6czNNqCSqb_lRojxQg=s64","userId":"15227785711110309391"}},"outputId":"9f32356f-3eaa-40b0-b6bb-4f85dde23b75","colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["# a)\n","success_rate1 = flip_bits(num_bits=7, num_epochs=250, HER='None')\n","success_rate2 = flip_bits(num_bits=7, num_epochs=150, HER='final')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Running bit flip environment with 7 bits and HER policy: None\n","Epoch: 0  Cumulative reward: -112.000000  Success rate: 0.0000 Mean loss: 0.1711\n","Epoch: 5  Cumulative reward: -112.000000  Success rate: 0.0000 Mean loss: 0.0058\n","Epoch: 10  Cumulative reward: -112.000000  Success rate: 0.0000 Mean loss: 0.0053\n","Epoch: 15  Cumulative reward: -112.000000  Success rate: 0.0000 Mean loss: 0.0036\n","Epoch: 20  Cumulative reward: -112.000000  Success rate: 0.0000 Mean loss: 0.0030\n","Epoch: 25  Cumulative reward: -112.000000  Success rate: 0.0000 Mean loss: 0.0036\n","Epoch: 30  Cumulative reward: -112.000000  Success rate: 0.0000 Mean loss: 0.0045\n","Epoch: 35  Cumulative reward: -111.000000  Success rate: 0.0625 Mean loss: 0.0037\n","Epoch: 40  Cumulative reward: -112.000000  Success rate: 0.0000 Mean loss: 0.0046\n","Epoch: 45  Cumulative reward: -112.000000  Success rate: 0.0000 Mean loss: 0.0047\n","Epoch: 50  Cumulative reward: -105.000000  Success rate: 0.1875 Mean loss: 0.0055\n","Epoch: 55  Cumulative reward: -109.000000  Success rate: 0.0625 Mean loss: 0.0066\n","Epoch: 60  Cumulative reward: -112.000000  Success rate: 0.0000 Mean loss: 0.0056\n","Epoch: 65  Cumulative reward: -112.000000  Success rate: 0.0000 Mean loss: 0.0059\n","Epoch: 70  Cumulative reward: -112.000000  Success rate: 0.0000 Mean loss: 0.0064\n","Epoch: 75  Cumulative reward: -112.000000  Success rate: 0.0000 Mean loss: 0.0071\n","Epoch: 80  Cumulative reward: -108.000000  Success rate: 0.1250 Mean loss: 0.0076\n","Epoch: 85  Cumulative reward: -110.000000  Success rate: 0.0625 Mean loss: 0.0099\n","Epoch: 90  Cumulative reward: -110.000000  Success rate: 0.0625 Mean loss: 0.0084\n","Epoch: 95  Cumulative reward: -109.000000  Success rate: 0.0625 Mean loss: 0.0091\n","Epoch: 100  Cumulative reward: -109.000000  Success rate: 0.0625 Mean loss: 0.0118\n","Epoch: 105  Cumulative reward: -108.000000  Success rate: 0.1250 Mean loss: 0.0108\n","Epoch: 110  Cumulative reward: -107.000000  Success rate: 0.1250 Mean loss: 0.0108\n","Epoch: 115  Cumulative reward: -109.000000  Success rate: 0.0625 Mean loss: 0.0117\n","Epoch: 120  Cumulative reward: -109.000000  Success rate: 0.0625 Mean loss: 0.0138\n","Epoch: 125  Cumulative reward: -110.000000  Success rate: 0.0625 Mean loss: 0.0159\n","Epoch: 130  Cumulative reward: -108.000000  Success rate: 0.0625 Mean loss: 0.0175\n","Epoch: 135  Cumulative reward: -109.000000  Success rate: 0.0625 Mean loss: 0.0153\n","Epoch: 140  Cumulative reward: -108.000000  Success rate: 0.1250 Mean loss: 0.0190\n","Epoch: 145  Cumulative reward: -91.000000  Success rate: 0.5000 Mean loss: 0.0220\n","Epoch: 150  Cumulative reward: -98.000000  Success rate: 0.3125 Mean loss: 0.0234\n","Epoch: 155  Cumulative reward: -93.000000  Success rate: 0.3750 Mean loss: 0.0278\n","Epoch: 160  Cumulative reward: -93.000000  Success rate: 0.4375 Mean loss: 0.0298\n","Epoch: 165  Cumulative reward: -79.000000  Success rate: 0.7500 Mean loss: 0.0300\n","Epoch: 170  Cumulative reward: -81.000000  Success rate: 0.8125 Mean loss: 0.0329\n","Epoch: 175  Cumulative reward: -81.000000  Success rate: 0.7500 Mean loss: 0.0373\n","Epoch: 180  Cumulative reward: -86.000000  Success rate: 0.6250 Mean loss: 0.0373\n","Epoch: 185  Cumulative reward: -76.000000  Success rate: 0.8750 Mean loss: 0.0415\n","Epoch: 190  Cumulative reward: -68.000000  Success rate: 1.0000 Mean loss: 0.0410\n","Epoch: 195  Cumulative reward: -70.000000  Success rate: 0.9375 Mean loss: 0.0455\n","Epoch: 200  Cumulative reward: -69.000000  Success rate: 1.0000 Mean loss: 0.0506\n","Epoch: 205  Cumulative reward: -71.000000  Success rate: 1.0000 Mean loss: 0.0462\n","Epoch: 210  Cumulative reward: -69.000000  Success rate: 1.0000 Mean loss: 0.0419\n","Epoch: 215  Cumulative reward: -72.000000  Success rate: 1.0000 Mean loss: 0.0478\n","Epoch: 220  Cumulative reward: -76.000000  Success rate: 1.0000 Mean loss: 0.0510\n","Epoch: 225  Cumulative reward: -72.000000  Success rate: 0.9375 Mean loss: 0.0531\n","Epoch: 230  Cumulative reward: -69.000000  Success rate: 1.0000 Mean loss: 0.0520\n","Epoch: 235  Cumulative reward: -73.000000  Success rate: 1.0000 Mean loss: 0.0524\n","Epoch: 240  Cumulative reward: -74.000000  Success rate: 1.0000 Mean loss: 0.0544\n","Epoch: 245  Cumulative reward: -71.000000  Success rate: 1.0000 Mean loss: 0.0559\n","Running bit flip environment with 7 bits and HER policy: final\n","Epoch: 0  Cumulative reward: -112.000000  Success rate: 0.0000 Mean loss: 0.1764\n","Epoch: 5  Cumulative reward: -112.000000  Success rate: 0.0000 Mean loss: 0.0623\n","Epoch: 10  Cumulative reward: -112.000000  Success rate: 0.0000 Mean loss: 0.0599\n","Epoch: 15  Cumulative reward: -101.000000  Success rate: 0.2500 Mean loss: 0.0694\n","Epoch: 20  Cumulative reward: -107.000000  Success rate: 0.1250 Mean loss: 0.0715\n","Epoch: 25  Cumulative reward: -104.000000  Success rate: 0.1875 Mean loss: 0.0818\n","Epoch: 30  Cumulative reward: -87.000000  Success rate: 0.6250 Mean loss: 0.0849\n","Epoch: 35  Cumulative reward: -89.000000  Success rate: 0.6250 Mean loss: 0.0861\n","Epoch: 40  Cumulative reward: -88.000000  Success rate: 0.6875 Mean loss: 0.0945\n","Epoch: 45  Cumulative reward: -81.000000  Success rate: 0.7500 Mean loss: 0.0979\n","Epoch: 50  Cumulative reward: -72.000000  Success rate: 1.0000 Mean loss: 0.1075\n","Epoch: 55  Cumulative reward: -80.000000  Success rate: 0.6875 Mean loss: 0.1095\n","Epoch: 60  Cumulative reward: -80.000000  Success rate: 0.7500 Mean loss: 0.1110\n","Epoch: 65  Cumulative reward: -71.000000  Success rate: 1.0000 Mean loss: 0.1199\n","Epoch: 70  Cumulative reward: -77.000000  Success rate: 0.8750 Mean loss: 0.1196\n","Epoch: 75  Cumulative reward: -67.000000  Success rate: 1.0000 Mean loss: 0.1239\n","Epoch: 80  Cumulative reward: -70.000000  Success rate: 0.9375 Mean loss: 0.1286\n","Epoch: 85  Cumulative reward: -68.000000  Success rate: 1.0000 Mean loss: 0.1290\n","Epoch: 90  Cumulative reward: -73.000000  Success rate: 0.9375 Mean loss: 0.1292\n","Epoch: 95  Cumulative reward: -77.000000  Success rate: 1.0000 Mean loss: 0.1313\n","Epoch: 100  Cumulative reward: -69.000000  Success rate: 1.0000 Mean loss: 0.1372\n","Epoch: 105  Cumulative reward: -72.000000  Success rate: 1.0000 Mean loss: 0.1372\n","Epoch: 110  Cumulative reward: -73.000000  Success rate: 1.0000 Mean loss: 0.1417\n","Epoch: 115  Cumulative reward: -70.000000  Success rate: 1.0000 Mean loss: 0.1376\n","Epoch: 120  Cumulative reward: -69.000000  Success rate: 1.0000 Mean loss: 0.1396\n","Epoch: 125  Cumulative reward: -74.000000  Success rate: 1.0000 Mean loss: 0.1391\n","Epoch: 130  Cumulative reward: -76.000000  Success rate: 1.0000 Mean loss: 0.1433\n","Epoch: 135  Cumulative reward: -73.000000  Success rate: 1.0000 Mean loss: 0.1511\n","Epoch: 140  Cumulative reward: -72.000000  Success rate: 1.0000 Mean loss: 0.1507\n","Epoch: 145  Cumulative reward: -75.000000  Success rate: 1.0000 Mean loss: 0.1450\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"LzEfK8iITloW","executionInfo":{"status":"ok","timestamp":1603601478414,"user_tz":420,"elapsed":402944,"user":{"displayName":"Yiheng Li","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ggo9FSuWqtcs815uMwgcd6czNNqCSqb_lRojxQg=s64","userId":"15227785711110309391"}},"outputId":"0fea1e86-ad3d-41ba-a5e8-cb1fabe37fe4","colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["# b)\n","success_rate3 = flip_bits(num_bits=15, num_epochs=500, HER='None')\n","success_rate4 = flip_bits(num_bits=15, num_epochs=500, HER='final')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Running bit flip environment with 15 bits and HER policy: None\n","Epoch: 0  Cumulative reward: -240.000000  Success rate: 0.0000 Mean loss: 0.1095\n","Epoch: 5  Cumulative reward: -240.000000  Success rate: 0.0000 Mean loss: 0.0041\n","Epoch: 10  Cumulative reward: -240.000000  Success rate: 0.0000 Mean loss: 0.0035\n","Epoch: 15  Cumulative reward: -240.000000  Success rate: 0.0000 Mean loss: 0.0014\n","Epoch: 20  Cumulative reward: -240.000000  Success rate: 0.0000 Mean loss: 0.0014\n","Epoch: 25  Cumulative reward: -240.000000  Success rate: 0.0000 Mean loss: 0.0013\n","Epoch: 30  Cumulative reward: -240.000000  Success rate: 0.0000 Mean loss: 0.0013\n","Epoch: 35  Cumulative reward: -240.000000  Success rate: 0.0000 Mean loss: 0.0015\n","Epoch: 40  Cumulative reward: -240.000000  Success rate: 0.0000 Mean loss: 0.0012\n","Epoch: 45  Cumulative reward: -240.000000  Success rate: 0.0000 Mean loss: 0.0014\n","Epoch: 50  Cumulative reward: -240.000000  Success rate: 0.0000 Mean loss: 0.0013\n","Epoch: 55  Cumulative reward: -240.000000  Success rate: 0.0000 Mean loss: 0.0015\n","Epoch: 60  Cumulative reward: -240.000000  Success rate: 0.0000 Mean loss: 0.0013\n","Epoch: 65  Cumulative reward: -240.000000  Success rate: 0.0000 Mean loss: 0.0015\n","Epoch: 70  Cumulative reward: -240.000000  Success rate: 0.0000 Mean loss: 0.0014\n","Epoch: 75  Cumulative reward: -240.000000  Success rate: 0.0000 Mean loss: 0.0017\n","Epoch: 80  Cumulative reward: -240.000000  Success rate: 0.0000 Mean loss: 0.0023\n","Epoch: 85  Cumulative reward: -240.000000  Success rate: 0.0000 Mean loss: 0.0015\n","Epoch: 90  Cumulative reward: -240.000000  Success rate: 0.0000 Mean loss: 0.0025\n","Epoch: 95  Cumulative reward: -240.000000  Success rate: 0.0000 Mean loss: 0.0020\n","Epoch: 100  Cumulative reward: -240.000000  Success rate: 0.0000 Mean loss: 0.0019\n","Epoch: 105  Cumulative reward: -240.000000  Success rate: 0.0000 Mean loss: 0.0017\n","Epoch: 110  Cumulative reward: -240.000000  Success rate: 0.0000 Mean loss: 0.0019\n","Epoch: 115  Cumulative reward: -240.000000  Success rate: 0.0000 Mean loss: 0.0027\n","Epoch: 120  Cumulative reward: -240.000000  Success rate: 0.0000 Mean loss: 0.0020\n","Epoch: 125  Cumulative reward: -240.000000  Success rate: 0.0000 Mean loss: 0.0024\n","Epoch: 130  Cumulative reward: -240.000000  Success rate: 0.0000 Mean loss: 0.0020\n","Epoch: 135  Cumulative reward: -240.000000  Success rate: 0.0000 Mean loss: 0.0025\n","Epoch: 140  Cumulative reward: -240.000000  Success rate: 0.0000 Mean loss: 0.0027\n","Epoch: 145  Cumulative reward: -240.000000  Success rate: 0.0000 Mean loss: 0.0021\n","Epoch: 150  Cumulative reward: -240.000000  Success rate: 0.0000 Mean loss: 0.0027\n","Epoch: 155  Cumulative reward: -240.000000  Success rate: 0.0000 Mean loss: 0.0032\n","Epoch: 160  Cumulative reward: -240.000000  Success rate: 0.0000 Mean loss: 0.0031\n","Epoch: 165  Cumulative reward: -240.000000  Success rate: 0.0000 Mean loss: 0.0027\n","Epoch: 170  Cumulative reward: -240.000000  Success rate: 0.0000 Mean loss: 0.0026\n","Epoch: 175  Cumulative reward: -240.000000  Success rate: 0.0000 Mean loss: 0.0026\n","Epoch: 180  Cumulative reward: -240.000000  Success rate: 0.0000 Mean loss: 0.0029\n","Epoch: 185  Cumulative reward: -240.000000  Success rate: 0.0000 Mean loss: 0.0037\n","Epoch: 190  Cumulative reward: -240.000000  Success rate: 0.0000 Mean loss: 0.0034\n","Epoch: 195  Cumulative reward: -240.000000  Success rate: 0.0000 Mean loss: 0.0034\n","Epoch: 200  Cumulative reward: -240.000000  Success rate: 0.0000 Mean loss: 0.0028\n","Epoch: 205  Cumulative reward: -240.000000  Success rate: 0.0000 Mean loss: 0.0034\n","Epoch: 210  Cumulative reward: -240.000000  Success rate: 0.0000 Mean loss: 0.0030\n","Epoch: 215  Cumulative reward: -240.000000  Success rate: 0.0000 Mean loss: 0.0031\n","Epoch: 220  Cumulative reward: -240.000000  Success rate: 0.0000 Mean loss: 0.0038\n","Epoch: 225  Cumulative reward: -240.000000  Success rate: 0.0000 Mean loss: 0.0031\n","Epoch: 230  Cumulative reward: -240.000000  Success rate: 0.0000 Mean loss: 0.0040\n","Epoch: 235  Cumulative reward: -240.000000  Success rate: 0.0000 Mean loss: 0.0033\n","Epoch: 240  Cumulative reward: -240.000000  Success rate: 0.0000 Mean loss: 0.0038\n","Epoch: 245  Cumulative reward: -240.000000  Success rate: 0.0000 Mean loss: 0.0044\n","Epoch: 250  Cumulative reward: -240.000000  Success rate: 0.0000 Mean loss: 0.0031\n","Epoch: 255  Cumulative reward: -240.000000  Success rate: 0.0000 Mean loss: 0.0039\n","Epoch: 260  Cumulative reward: -240.000000  Success rate: 0.0000 Mean loss: 0.0037\n","Epoch: 265  Cumulative reward: -240.000000  Success rate: 0.0000 Mean loss: 0.0040\n","Epoch: 270  Cumulative reward: -240.000000  Success rate: 0.0000 Mean loss: 0.0035\n","Epoch: 275  Cumulative reward: -240.000000  Success rate: 0.0000 Mean loss: 0.0043\n","Epoch: 280  Cumulative reward: -240.000000  Success rate: 0.0000 Mean loss: 0.0033\n","Epoch: 285  Cumulative reward: -240.000000  Success rate: 0.0000 Mean loss: 0.0042\n","Epoch: 290  Cumulative reward: -240.000000  Success rate: 0.0000 Mean loss: 0.0048\n","Epoch: 295  Cumulative reward: -240.000000  Success rate: 0.0000 Mean loss: 0.0038\n","Epoch: 300  Cumulative reward: -240.000000  Success rate: 0.0000 Mean loss: 0.0039\n","Epoch: 305  Cumulative reward: -240.000000  Success rate: 0.0000 Mean loss: 0.0035\n","Epoch: 310  Cumulative reward: -240.000000  Success rate: 0.0000 Mean loss: 0.0038\n","Epoch: 315  Cumulative reward: -240.000000  Success rate: 0.0000 Mean loss: 0.0051\n","Epoch: 320  Cumulative reward: -240.000000  Success rate: 0.0000 Mean loss: 0.0038\n","Epoch: 325  Cumulative reward: -240.000000  Success rate: 0.0000 Mean loss: 0.0051\n","Epoch: 330  Cumulative reward: -240.000000  Success rate: 0.0000 Mean loss: 0.0046\n","Epoch: 335  Cumulative reward: -240.000000  Success rate: 0.0000 Mean loss: 0.0038\n","Epoch: 340  Cumulative reward: -240.000000  Success rate: 0.0000 Mean loss: 0.0046\n","Epoch: 345  Cumulative reward: -240.000000  Success rate: 0.0000 Mean loss: 0.0042\n","Epoch: 350  Cumulative reward: -240.000000  Success rate: 0.0000 Mean loss: 0.0040\n","Epoch: 355  Cumulative reward: -240.000000  Success rate: 0.0000 Mean loss: 0.0041\n","Epoch: 360  Cumulative reward: -240.000000  Success rate: 0.0000 Mean loss: 0.0041\n","Epoch: 365  Cumulative reward: -240.000000  Success rate: 0.0000 Mean loss: 0.0051\n","Epoch: 370  Cumulative reward: -240.000000  Success rate: 0.0000 Mean loss: 0.0045\n","Epoch: 375  Cumulative reward: -240.000000  Success rate: 0.0000 Mean loss: 0.0044\n","Epoch: 380  Cumulative reward: -240.000000  Success rate: 0.0000 Mean loss: 0.0045\n","Epoch: 385  Cumulative reward: -240.000000  Success rate: 0.0000 Mean loss: 0.0048\n","Epoch: 390  Cumulative reward: -240.000000  Success rate: 0.0000 Mean loss: 0.0048\n","Epoch: 395  Cumulative reward: -240.000000  Success rate: 0.0000 Mean loss: 0.0040\n","Epoch: 400  Cumulative reward: -240.000000  Success rate: 0.0000 Mean loss: 0.0044\n","Epoch: 405  Cumulative reward: -240.000000  Success rate: 0.0000 Mean loss: 0.0043\n","Epoch: 410  Cumulative reward: -240.000000  Success rate: 0.0000 Mean loss: 0.0043\n","Epoch: 415  Cumulative reward: -240.000000  Success rate: 0.0000 Mean loss: 0.0053\n","Epoch: 420  Cumulative reward: -240.000000  Success rate: 0.0000 Mean loss: 0.0044\n","Epoch: 425  Cumulative reward: -240.000000  Success rate: 0.0000 Mean loss: 0.0051\n","Epoch: 430  Cumulative reward: -240.000000  Success rate: 0.0000 Mean loss: 0.0040\n","Epoch: 435  Cumulative reward: -240.000000  Success rate: 0.0000 Mean loss: 0.0052\n","Epoch: 440  Cumulative reward: -240.000000  Success rate: 0.0000 Mean loss: 0.0048\n","Epoch: 445  Cumulative reward: -240.000000  Success rate: 0.0000 Mean loss: 0.0049\n","Epoch: 450  Cumulative reward: -240.000000  Success rate: 0.0000 Mean loss: 0.0046\n","Epoch: 455  Cumulative reward: -240.000000  Success rate: 0.0000 Mean loss: 0.0045\n","Epoch: 460  Cumulative reward: -240.000000  Success rate: 0.0000 Mean loss: 0.0050\n","Epoch: 465  Cumulative reward: -240.000000  Success rate: 0.0000 Mean loss: 0.0053\n","Epoch: 470  Cumulative reward: -240.000000  Success rate: 0.0000 Mean loss: 0.0052\n","Epoch: 475  Cumulative reward: -240.000000  Success rate: 0.0000 Mean loss: 0.0046\n","Epoch: 480  Cumulative reward: -240.000000  Success rate: 0.0000 Mean loss: 0.0047\n","Epoch: 485  Cumulative reward: -240.000000  Success rate: 0.0000 Mean loss: 0.0045\n","Epoch: 490  Cumulative reward: -240.000000  Success rate: 0.0000 Mean loss: 0.0056\n","Epoch: 495  Cumulative reward: -240.000000  Success rate: 0.0000 Mean loss: 0.0054\n","Running bit flip environment with 15 bits and HER policy: final\n","Epoch: 0  Cumulative reward: -240.000000  Success rate: 0.0000 Mean loss: 0.1011\n","Epoch: 5  Cumulative reward: -240.000000  Success rate: 0.0000 Mean loss: 0.0222\n","Epoch: 10  Cumulative reward: -240.000000  Success rate: 0.0000 Mean loss: 0.0272\n","Epoch: 15  Cumulative reward: -240.000000  Success rate: 0.0000 Mean loss: 0.0349\n","Epoch: 20  Cumulative reward: -240.000000  Success rate: 0.0000 Mean loss: 0.0356\n","Epoch: 25  Cumulative reward: -240.000000  Success rate: 0.0000 Mean loss: 0.0313\n","Epoch: 30  Cumulative reward: -240.000000  Success rate: 0.0000 Mean loss: 0.0310\n","Epoch: 35  Cumulative reward: -240.000000  Success rate: 0.0000 Mean loss: 0.0373\n","Epoch: 40  Cumulative reward: -240.000000  Success rate: 0.0000 Mean loss: 0.0362\n","Epoch: 45  Cumulative reward: -240.000000  Success rate: 0.0000 Mean loss: 0.0370\n","Epoch: 50  Cumulative reward: -240.000000  Success rate: 0.0000 Mean loss: 0.0377\n","Epoch: 55  Cumulative reward: -240.000000  Success rate: 0.0000 Mean loss: 0.0367\n","Epoch: 60  Cumulative reward: -240.000000  Success rate: 0.0000 Mean loss: 0.0339\n","Epoch: 65  Cumulative reward: -240.000000  Success rate: 0.0000 Mean loss: 0.0326\n","Epoch: 70  Cumulative reward: -240.000000  Success rate: 0.0000 Mean loss: 0.0310\n","Epoch: 75  Cumulative reward: -240.000000  Success rate: 0.0000 Mean loss: 0.0336\n","Epoch: 80  Cumulative reward: -240.000000  Success rate: 0.0000 Mean loss: 0.0368\n","Epoch: 85  Cumulative reward: -240.000000  Success rate: 0.0000 Mean loss: 0.0378\n","Epoch: 90  Cumulative reward: -240.000000  Success rate: 0.0000 Mean loss: 0.0404\n","Epoch: 95  Cumulative reward: -240.000000  Success rate: 0.0000 Mean loss: 0.0374\n","Epoch: 100  Cumulative reward: -240.000000  Success rate: 0.0000 Mean loss: 0.0391\n","Epoch: 105  Cumulative reward: -240.000000  Success rate: 0.0000 Mean loss: 0.0324\n","Epoch: 110  Cumulative reward: -235.000000  Success rate: 0.0625 Mean loss: 0.0354\n","Epoch: 115  Cumulative reward: -240.000000  Success rate: 0.0000 Mean loss: 0.0387\n","Epoch: 120  Cumulative reward: -240.000000  Success rate: 0.0000 Mean loss: 0.0396\n","Epoch: 125  Cumulative reward: -240.000000  Success rate: 0.0000 Mean loss: 0.0337\n","Epoch: 130  Cumulative reward: -240.000000  Success rate: 0.0000 Mean loss: 0.0391\n","Epoch: 135  Cumulative reward: -240.000000  Success rate: 0.0000 Mean loss: 0.0389\n","Epoch: 140  Cumulative reward: -240.000000  Success rate: 0.0000 Mean loss: 0.0362\n","Epoch: 145  Cumulative reward: -240.000000  Success rate: 0.0000 Mean loss: 0.0376\n","Epoch: 150  Cumulative reward: -240.000000  Success rate: 0.0000 Mean loss: 0.0351\n","Epoch: 155  Cumulative reward: -240.000000  Success rate: 0.0000 Mean loss: 0.0350\n","Epoch: 160  Cumulative reward: -240.000000  Success rate: 0.0000 Mean loss: 0.0352\n","Epoch: 165  Cumulative reward: -240.000000  Success rate: 0.0000 Mean loss: 0.0427\n","Epoch: 170  Cumulative reward: -240.000000  Success rate: 0.0000 Mean loss: 0.0389\n","Epoch: 175  Cumulative reward: -240.000000  Success rate: 0.0000 Mean loss: 0.0406\n","Epoch: 180  Cumulative reward: -240.000000  Success rate: 0.0000 Mean loss: 0.0421\n","Epoch: 185  Cumulative reward: -240.000000  Success rate: 0.0000 Mean loss: 0.0379\n","Epoch: 190  Cumulative reward: -240.000000  Success rate: 0.0000 Mean loss: 0.0364\n","Epoch: 195  Cumulative reward: -235.000000  Success rate: 0.0625 Mean loss: 0.0360\n","Epoch: 200  Cumulative reward: -240.000000  Success rate: 0.0000 Mean loss: 0.0340\n","Epoch: 205  Cumulative reward: -240.000000  Success rate: 0.0000 Mean loss: 0.0385\n","Epoch: 210  Cumulative reward: -240.000000  Success rate: 0.0000 Mean loss: 0.0394\n","Epoch: 215  Cumulative reward: -240.000000  Success rate: 0.0000 Mean loss: 0.0428\n","Epoch: 220  Cumulative reward: -234.000000  Success rate: 0.0625 Mean loss: 0.0393\n","Epoch: 225  Cumulative reward: -240.000000  Success rate: 0.0000 Mean loss: 0.0383\n","Epoch: 230  Cumulative reward: -240.000000  Success rate: 0.0000 Mean loss: 0.0415\n","Epoch: 235  Cumulative reward: -234.000000  Success rate: 0.0625 Mean loss: 0.0405\n","Epoch: 240  Cumulative reward: -240.000000  Success rate: 0.0000 Mean loss: 0.0365\n","Epoch: 245  Cumulative reward: -230.000000  Success rate: 0.1250 Mean loss: 0.0366\n","Epoch: 250  Cumulative reward: -240.000000  Success rate: 0.0000 Mean loss: 0.0386\n","Epoch: 255  Cumulative reward: -240.000000  Success rate: 0.0000 Mean loss: 0.0427\n","Epoch: 260  Cumulative reward: -240.000000  Success rate: 0.0000 Mean loss: 0.0414\n","Epoch: 265  Cumulative reward: -235.000000  Success rate: 0.0625 Mean loss: 0.0384\n","Epoch: 270  Cumulative reward: -240.000000  Success rate: 0.0000 Mean loss: 0.0401\n","Epoch: 275  Cumulative reward: -240.000000  Success rate: 0.0000 Mean loss: 0.0462\n","Epoch: 280  Cumulative reward: -236.000000  Success rate: 0.0625 Mean loss: 0.0402\n","Epoch: 285  Cumulative reward: -240.000000  Success rate: 0.0000 Mean loss: 0.0367\n","Epoch: 290  Cumulative reward: -237.000000  Success rate: 0.0625 Mean loss: 0.0408\n","Epoch: 295  Cumulative reward: -231.000000  Success rate: 0.1250 Mean loss: 0.0427\n","Epoch: 300  Cumulative reward: -236.000000  Success rate: 0.0625 Mean loss: 0.0406\n","Epoch: 305  Cumulative reward: -240.000000  Success rate: 0.0000 Mean loss: 0.0385\n","Epoch: 310  Cumulative reward: -240.000000  Success rate: 0.0000 Mean loss: 0.0428\n","Epoch: 315  Cumulative reward: -240.000000  Success rate: 0.0000 Mean loss: 0.0420\n","Epoch: 320  Cumulative reward: -240.000000  Success rate: 0.0000 Mean loss: 0.0480\n","Epoch: 325  Cumulative reward: -239.000000  Success rate: 0.0625 Mean loss: 0.0498\n","Epoch: 330  Cumulative reward: -240.000000  Success rate: 0.0000 Mean loss: 0.0400\n","Epoch: 335  Cumulative reward: -227.000000  Success rate: 0.1875 Mean loss: 0.0410\n","Epoch: 340  Cumulative reward: -236.000000  Success rate: 0.0625 Mean loss: 0.0423\n","Epoch: 345  Cumulative reward: -240.000000  Success rate: 0.0000 Mean loss: 0.0438\n","Epoch: 350  Cumulative reward: -235.000000  Success rate: 0.0625 Mean loss: 0.0433\n","Epoch: 355  Cumulative reward: -236.000000  Success rate: 0.0625 Mean loss: 0.0428\n","Epoch: 360  Cumulative reward: -234.000000  Success rate: 0.0625 Mean loss: 0.0451\n","Epoch: 365  Cumulative reward: -240.000000  Success rate: 0.0000 Mean loss: 0.0476\n","Epoch: 370  Cumulative reward: -236.000000  Success rate: 0.0625 Mean loss: 0.0468\n","Epoch: 375  Cumulative reward: -235.000000  Success rate: 0.0625 Mean loss: 0.0448\n","Epoch: 380  Cumulative reward: -219.000000  Success rate: 0.2500 Mean loss: 0.0452\n","Epoch: 385  Cumulative reward: -235.000000  Success rate: 0.0625 Mean loss: 0.0445\n","Epoch: 390  Cumulative reward: -232.000000  Success rate: 0.1250 Mean loss: 0.0478\n","Epoch: 395  Cumulative reward: -236.000000  Success rate: 0.0625 Mean loss: 0.0492\n","Epoch: 400  Cumulative reward: -234.000000  Success rate: 0.0625 Mean loss: 0.0475\n","Epoch: 405  Cumulative reward: -230.000000  Success rate: 0.1250 Mean loss: 0.0455\n","Epoch: 410  Cumulative reward: -240.000000  Success rate: 0.0000 Mean loss: 0.0493\n","Epoch: 415  Cumulative reward: -235.000000  Success rate: 0.0625 Mean loss: 0.0488\n","Epoch: 420  Cumulative reward: -226.000000  Success rate: 0.1875 Mean loss: 0.0473\n","Epoch: 425  Cumulative reward: -240.000000  Success rate: 0.0000 Mean loss: 0.0484\n","Epoch: 430  Cumulative reward: -227.000000  Success rate: 0.1875 Mean loss: 0.0475\n","Epoch: 435  Cumulative reward: -229.000000  Success rate: 0.1250 Mean loss: 0.0523\n","Epoch: 440  Cumulative reward: -235.000000  Success rate: 0.0625 Mean loss: 0.0519\n","Epoch: 445  Cumulative reward: -219.000000  Success rate: 0.2500 Mean loss: 0.0526\n","Epoch: 450  Cumulative reward: -232.000000  Success rate: 0.0625 Mean loss: 0.0547\n","Epoch: 455  Cumulative reward: -227.000000  Success rate: 0.1875 Mean loss: 0.0463\n","Epoch: 460  Cumulative reward: -230.000000  Success rate: 0.1250 Mean loss: 0.0514\n","Epoch: 465  Cumulative reward: -231.000000  Success rate: 0.1250 Mean loss: 0.0539\n","Epoch: 470  Cumulative reward: -219.000000  Success rate: 0.2500 Mean loss: 0.0494\n","Epoch: 475  Cumulative reward: -229.000000  Success rate: 0.1250 Mean loss: 0.0486\n","Epoch: 480  Cumulative reward: -231.000000  Success rate: 0.1250 Mean loss: 0.0524\n","Epoch: 485  Cumulative reward: -225.000000  Success rate: 0.1875 Mean loss: 0.0521\n","Epoch: 490  Cumulative reward: -237.000000  Success rate: 0.0625 Mean loss: 0.0494\n","Epoch: 495  Cumulative reward: -240.000000  Success rate: 0.0000 Mean loss: 0.0558\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"_Fp0cE1nTnzA","executionInfo":{"status":"ok","timestamp":1603602739678,"user_tz":420,"elapsed":1086574,"user":{"displayName":"Yiheng Li","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ggo9FSuWqtcs815uMwgcd6czNNqCSqb_lRojxQg=s64","userId":"15227785711110309391"}},"outputId":"f31d13c7-7675-452e-f088-a648703aa8e2","colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["# c)\n","success_rate5 = flip_bits(num_bits=25, num_epochs=1000, HER='None')\n","success_rate6 = flip_bits(num_bits=25, num_epochs=1000, HER='final')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Running bit flip environment with 25 bits and HER policy: None\n","Epoch: 0  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.1043\n","Epoch: 5  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0041\n","Epoch: 10  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0028\n","Epoch: 15  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0018\n","Epoch: 20  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0015\n","Epoch: 25  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0011\n","Epoch: 30  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0016\n","Epoch: 35  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0012\n","Epoch: 40  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0013\n","Epoch: 45  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0014\n","Epoch: 50  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0013\n","Epoch: 55  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0011\n","Epoch: 60  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0015\n","Epoch: 65  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0021\n","Epoch: 70  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0017\n","Epoch: 75  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0020\n","Epoch: 80  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0018\n","Epoch: 85  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0020\n","Epoch: 90  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0023\n","Epoch: 95  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0022\n","Epoch: 100  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0023\n","Epoch: 105  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0027\n","Epoch: 110  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0029\n","Epoch: 115  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0025\n","Epoch: 120  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0031\n","Epoch: 125  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0029\n","Epoch: 130  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0028\n","Epoch: 135  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0034\n","Epoch: 140  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0033\n","Epoch: 145  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0029\n","Epoch: 150  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0035\n","Epoch: 155  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0036\n","Epoch: 160  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0032\n","Epoch: 165  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0035\n","Epoch: 170  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0035\n","Epoch: 175  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0049\n","Epoch: 180  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0044\n","Epoch: 185  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0037\n","Epoch: 190  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0035\n","Epoch: 195  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0038\n","Epoch: 200  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0043\n","Epoch: 205  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0050\n","Epoch: 210  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0049\n","Epoch: 215  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0052\n","Epoch: 220  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0052\n","Epoch: 225  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0040\n","Epoch: 230  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0045\n","Epoch: 235  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0054\n","Epoch: 240  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0056\n","Epoch: 245  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0069\n","Epoch: 250  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0053\n","Epoch: 255  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0065\n","Epoch: 260  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0053\n","Epoch: 265  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0052\n","Epoch: 270  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0049\n","Epoch: 275  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0055\n","Epoch: 280  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0075\n","Epoch: 285  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0063\n","Epoch: 290  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0051\n","Epoch: 295  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0062\n","Epoch: 300  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0055\n","Epoch: 305  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0064\n","Epoch: 310  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0073\n","Epoch: 315  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0068\n","Epoch: 320  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0060\n","Epoch: 325  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0064\n","Epoch: 330  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0057\n","Epoch: 335  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0065\n","Epoch: 340  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0073\n","Epoch: 345  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0067\n","Epoch: 350  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0054\n","Epoch: 355  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0058\n","Epoch: 360  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0071\n","Epoch: 365  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0070\n","Epoch: 370  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0066\n","Epoch: 375  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0063\n","Epoch: 380  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0078\n","Epoch: 385  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0068\n","Epoch: 390  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0069\n","Epoch: 395  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0070\n","Epoch: 400  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0106\n","Epoch: 405  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0119\n","Epoch: 410  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0072\n","Epoch: 415  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0087\n","Epoch: 420  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0072\n","Epoch: 425  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0071\n","Epoch: 430  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0072\n","Epoch: 435  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0078\n","Epoch: 440  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0077\n","Epoch: 445  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0072\n","Epoch: 450  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0081\n","Epoch: 455  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0070\n","Epoch: 460  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0103\n","Epoch: 465  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0082\n","Epoch: 470  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0073\n","Epoch: 475  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0075\n","Epoch: 480  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0084\n","Epoch: 485  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0077\n","Epoch: 490  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0080\n","Epoch: 495  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0086\n","Epoch: 500  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0091\n","Epoch: 505  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0084\n","Epoch: 510  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0075\n","Epoch: 515  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0085\n","Epoch: 520  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0078\n","Epoch: 525  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0079\n","Epoch: 530  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0076\n","Epoch: 535  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0081\n","Epoch: 540  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0081\n","Epoch: 545  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0078\n","Epoch: 550  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0095\n","Epoch: 555  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0102\n","Epoch: 560  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0078\n","Epoch: 565  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0090\n","Epoch: 570  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0083\n","Epoch: 575  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0090\n","Epoch: 580  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0079\n","Epoch: 585  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0133\n","Epoch: 590  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0095\n","Epoch: 595  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0105\n","Epoch: 600  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0076\n","Epoch: 605  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0095\n","Epoch: 610  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0092\n","Epoch: 615  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0094\n","Epoch: 620  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0104\n","Epoch: 625  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0083\n","Epoch: 630  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0104\n","Epoch: 635  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0087\n","Epoch: 640  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0078\n","Epoch: 645  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0089\n","Epoch: 650  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0103\n","Epoch: 655  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0104\n","Epoch: 660  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0112\n","Epoch: 665  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0103\n","Epoch: 670  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0089\n","Epoch: 675  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0093\n","Epoch: 680  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0111\n","Epoch: 685  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0094\n","Epoch: 690  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0099\n","Epoch: 695  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0084\n","Epoch: 700  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0106\n","Epoch: 705  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0101\n","Epoch: 710  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0091\n","Epoch: 715  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0087\n","Epoch: 720  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0085\n","Epoch: 725  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0092\n","Epoch: 730  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0089\n","Epoch: 735  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0096\n","Epoch: 740  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0089\n","Epoch: 745  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0085\n","Epoch: 750  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0099\n","Epoch: 755  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0097\n","Epoch: 760  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0101\n","Epoch: 765  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0095\n","Epoch: 770  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0106\n","Epoch: 775  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0098\n","Epoch: 780  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0089\n","Epoch: 785  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0097\n","Epoch: 790  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0105\n","Epoch: 795  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0101\n","Epoch: 800  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0098\n","Epoch: 805  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0090\n","Epoch: 810  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0095\n","Epoch: 815  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0097\n","Epoch: 820  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0110\n","Epoch: 825  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0121\n","Epoch: 830  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0104\n","Epoch: 835  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0096\n","Epoch: 840  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0157\n","Epoch: 845  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0117\n","Epoch: 850  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0105\n","Epoch: 855  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0116\n","Epoch: 860  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0111\n","Epoch: 865  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0105\n","Epoch: 870  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0085\n","Epoch: 875  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0098\n","Epoch: 880  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0102\n","Epoch: 885  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0113\n","Epoch: 890  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0100\n","Epoch: 895  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0093\n","Epoch: 900  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0100\n","Epoch: 905  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0106\n","Epoch: 910  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0097\n","Epoch: 915  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0113\n","Epoch: 920  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0094\n","Epoch: 925  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0097\n","Epoch: 930  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0129\n","Epoch: 935  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0107\n","Epoch: 940  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0110\n","Epoch: 945  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0106\n","Epoch: 950  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0105\n","Epoch: 955  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0101\n","Epoch: 960  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0103\n","Epoch: 965  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0114\n","Epoch: 970  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0111\n","Epoch: 975  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0104\n","Epoch: 980  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0108\n","Epoch: 985  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0098\n","Epoch: 990  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0124\n","Epoch: 995  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0110\n","Running bit flip environment with 25 bits and HER policy: final\n","Epoch: 0  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.1698\n","Epoch: 5  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0071\n","Epoch: 10  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0083\n","Epoch: 15  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0147\n","Epoch: 20  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0212\n","Epoch: 25  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0262\n","Epoch: 30  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0273\n","Epoch: 35  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0258\n","Epoch: 40  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0261\n","Epoch: 45  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0252\n","Epoch: 50  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0245\n","Epoch: 55  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0242\n","Epoch: 60  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0239\n","Epoch: 65  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0236\n","Epoch: 70  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0248\n","Epoch: 75  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0269\n","Epoch: 80  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0250\n","Epoch: 85  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0281\n","Epoch: 90  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0222\n","Epoch: 95  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0249\n","Epoch: 100  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0243\n","Epoch: 105  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0241\n","Epoch: 110  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0270\n","Epoch: 115  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0267\n","Epoch: 120  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0247\n","Epoch: 125  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0254\n","Epoch: 130  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0268\n","Epoch: 135  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0264\n","Epoch: 140  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0261\n","Epoch: 145  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0268\n","Epoch: 150  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0251\n","Epoch: 155  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0300\n","Epoch: 160  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0251\n","Epoch: 165  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0282\n","Epoch: 170  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0262\n","Epoch: 175  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0304\n","Epoch: 180  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0322\n","Epoch: 185  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0274\n","Epoch: 190  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0300\n","Epoch: 195  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0266\n","Epoch: 200  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0279\n","Epoch: 205  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0259\n","Epoch: 210  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0316\n","Epoch: 215  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0305\n","Epoch: 220  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0279\n","Epoch: 225  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0266\n","Epoch: 230  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0331\n","Epoch: 235  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0246\n","Epoch: 240  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0306\n","Epoch: 245  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0246\n","Epoch: 250  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0251\n","Epoch: 255  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0363\n","Epoch: 260  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0292\n","Epoch: 265  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0293\n","Epoch: 270  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0275\n","Epoch: 275  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0282\n","Epoch: 280  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0277\n","Epoch: 285  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0321\n","Epoch: 290  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0324\n","Epoch: 295  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0321\n","Epoch: 300  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0337\n","Epoch: 305  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0332\n","Epoch: 310  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0317\n","Epoch: 315  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0345\n","Epoch: 320  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0311\n","Epoch: 325  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0300\n","Epoch: 330  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0321\n","Epoch: 335  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0325\n","Epoch: 340  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0277\n","Epoch: 345  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0313\n","Epoch: 350  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0310\n","Epoch: 355  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0361\n","Epoch: 360  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0282\n","Epoch: 365  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0321\n","Epoch: 370  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0348\n","Epoch: 375  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0314\n","Epoch: 380  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0355\n","Epoch: 385  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0292\n","Epoch: 390  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0337\n","Epoch: 395  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0308\n","Epoch: 400  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0272\n","Epoch: 405  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0366\n","Epoch: 410  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0322\n","Epoch: 415  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0264\n","Epoch: 420  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0292\n","Epoch: 425  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0316\n","Epoch: 430  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0291\n","Epoch: 435  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0297\n","Epoch: 440  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0288\n","Epoch: 445  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0295\n","Epoch: 450  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0322\n","Epoch: 455  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0351\n","Epoch: 460  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0307\n","Epoch: 465  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0322\n","Epoch: 470  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0355\n","Epoch: 475  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0291\n","Epoch: 480  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0334\n","Epoch: 485  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0324\n","Epoch: 490  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0320\n","Epoch: 495  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0350\n","Epoch: 500  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0357\n","Epoch: 505  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0350\n","Epoch: 510  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0290\n","Epoch: 515  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0338\n","Epoch: 520  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0346\n","Epoch: 525  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0329\n","Epoch: 530  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0301\n","Epoch: 535  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0362\n","Epoch: 540  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0303\n","Epoch: 545  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0375\n","Epoch: 550  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0299\n","Epoch: 555  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0321\n","Epoch: 560  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0331\n","Epoch: 565  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0341\n","Epoch: 570  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0338\n","Epoch: 575  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0334\n","Epoch: 580  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0310\n","Epoch: 585  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0334\n","Epoch: 590  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0309\n","Epoch: 595  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0350\n","Epoch: 600  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0364\n","Epoch: 605  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0319\n","Epoch: 610  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0315\n","Epoch: 615  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0434\n","Epoch: 620  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0352\n","Epoch: 625  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0396\n","Epoch: 630  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0320\n","Epoch: 635  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0334\n","Epoch: 640  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0369\n","Epoch: 645  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0334\n","Epoch: 650  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0304\n","Epoch: 655  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0361\n","Epoch: 660  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0363\n","Epoch: 665  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0319\n","Epoch: 670  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0316\n","Epoch: 675  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0361\n","Epoch: 680  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0300\n","Epoch: 685  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0308\n","Epoch: 690  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0302\n","Epoch: 695  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0368\n","Epoch: 700  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0312\n","Epoch: 705  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0326\n","Epoch: 710  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0321\n","Epoch: 715  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0322\n","Epoch: 720  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0358\n","Epoch: 725  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0348\n","Epoch: 730  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0326\n","Epoch: 735  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0358\n","Epoch: 740  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0346\n","Epoch: 745  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0337\n","Epoch: 750  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0323\n","Epoch: 755  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0357\n","Epoch: 760  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0354\n","Epoch: 765  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0301\n","Epoch: 770  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0367\n","Epoch: 775  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0327\n","Epoch: 780  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0396\n","Epoch: 785  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0346\n","Epoch: 790  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0339\n","Epoch: 795  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0365\n","Epoch: 800  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0323\n","Epoch: 805  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0374\n","Epoch: 810  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0421\n","Epoch: 815  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0311\n","Epoch: 820  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0347\n","Epoch: 825  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0338\n","Epoch: 830  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0349\n","Epoch: 835  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0346\n","Epoch: 840  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0336\n","Epoch: 845  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0363\n","Epoch: 850  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0404\n","Epoch: 855  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0372\n","Epoch: 860  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0408\n","Epoch: 865  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0358\n","Epoch: 870  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0376\n","Epoch: 875  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0323\n","Epoch: 880  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0447\n","Epoch: 885  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0383\n","Epoch: 890  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0345\n","Epoch: 895  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0355\n","Epoch: 900  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0415\n","Epoch: 905  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0300\n","Epoch: 910  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0315\n","Epoch: 915  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0354\n","Epoch: 920  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0323\n","Epoch: 925  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0317\n","Epoch: 930  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0339\n","Epoch: 935  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0336\n","Epoch: 940  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0354\n","Epoch: 945  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0402\n","Epoch: 950  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0300\n","Epoch: 955  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0368\n","Epoch: 960  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0323\n","Epoch: 965  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0370\n","Epoch: 970  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0396\n","Epoch: 975  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0295\n","Epoch: 980  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0310\n","Epoch: 985  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0427\n","Epoch: 990  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0380\n","Epoch: 995  Cumulative reward: -400.000000  Success rate: 0.0000 Mean loss: 0.0412\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"8FUFrso5T_Zo","executionInfo":{"status":"ok","timestamp":1603603715403,"user_tz":420,"elapsed":435726,"user":{"displayName":"Yiheng Li","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ggo9FSuWqtcs815uMwgcd6czNNqCSqb_lRojxQg=s64","userId":"15227785711110309391"}},"outputId":"a841eab7-152c-414c-f507-2a844818105e","colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["# d)\n","success_rate7 = flip_bits(num_bits=15, num_epochs=500, HER='future')\n","success_rate8 = flip_bits(num_bits=15, num_epochs=500, HER='random')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Running bit flip environment with 15 bits and HER policy: future\n","Epoch: 0  Cumulative reward: -240.000000  Success rate: 0.0000 Mean loss: 0.1140\n","Epoch: 5  Cumulative reward: -240.000000  Success rate: 0.0000 Mean loss: 0.0595\n","Epoch: 10  Cumulative reward: -240.000000  Success rate: 0.0000 Mean loss: 0.0968\n","Epoch: 15  Cumulative reward: -240.000000  Success rate: 0.0000 Mean loss: 0.1291\n","Epoch: 20  Cumulative reward: -240.000000  Success rate: 0.0000 Mean loss: 0.1268\n","Epoch: 25  Cumulative reward: -240.000000  Success rate: 0.0000 Mean loss: 0.1295\n","Epoch: 30  Cumulative reward: -240.000000  Success rate: 0.0000 Mean loss: 0.1265\n","Epoch: 35  Cumulative reward: -240.000000  Success rate: 0.0000 Mean loss: 0.1298\n","Epoch: 40  Cumulative reward: -240.000000  Success rate: 0.0000 Mean loss: 0.1270\n","Epoch: 45  Cumulative reward: -240.000000  Success rate: 0.0000 Mean loss: 0.1252\n","Epoch: 50  Cumulative reward: -236.000000  Success rate: 0.0625 Mean loss: 0.1294\n","Epoch: 55  Cumulative reward: -235.000000  Success rate: 0.0625 Mean loss: 0.1219\n","Epoch: 60  Cumulative reward: -240.000000  Success rate: 0.0000 Mean loss: 0.1226\n","Epoch: 65  Cumulative reward: -240.000000  Success rate: 0.0000 Mean loss: 0.1210\n","Epoch: 70  Cumulative reward: -239.000000  Success rate: 0.0625 Mean loss: 0.1210\n","Epoch: 75  Cumulative reward: -240.000000  Success rate: 0.0000 Mean loss: 0.1276\n","Epoch: 80  Cumulative reward: -230.000000  Success rate: 0.1250 Mean loss: 0.1224\n","Epoch: 85  Cumulative reward: -230.000000  Success rate: 0.1250 Mean loss: 0.1213\n","Epoch: 90  Cumulative reward: -222.000000  Success rate: 0.1875 Mean loss: 0.1231\n","Epoch: 95  Cumulative reward: -240.000000  Success rate: 0.0000 Mean loss: 0.1174\n","Epoch: 100  Cumulative reward: -230.000000  Success rate: 0.1250 Mean loss: 0.1197\n","Epoch: 105  Cumulative reward: -224.000000  Success rate: 0.1875 Mean loss: 0.1234\n","Epoch: 110  Cumulative reward: -225.000000  Success rate: 0.1875 Mean loss: 0.1285\n","Epoch: 115  Cumulative reward: -234.000000  Success rate: 0.0625 Mean loss: 0.1232\n","Epoch: 120  Cumulative reward: -231.000000  Success rate: 0.1250 Mean loss: 0.1261\n","Epoch: 125  Cumulative reward: -231.000000  Success rate: 0.1250 Mean loss: 0.1239\n","Epoch: 130  Cumulative reward: -226.000000  Success rate: 0.1875 Mean loss: 0.1269\n","Epoch: 135  Cumulative reward: -215.000000  Success rate: 0.3125 Mean loss: 0.1252\n","Epoch: 140  Cumulative reward: -233.000000  Success rate: 0.0625 Mean loss: 0.1276\n","Epoch: 145  Cumulative reward: -212.000000  Success rate: 0.3750 Mean loss: 0.1320\n","Epoch: 150  Cumulative reward: -204.000000  Success rate: 0.4375 Mean loss: 0.1268\n","Epoch: 155  Cumulative reward: -222.000000  Success rate: 0.2500 Mean loss: 0.1311\n","Epoch: 160  Cumulative reward: -217.000000  Success rate: 0.3125 Mean loss: 0.1303\n","Epoch: 165  Cumulative reward: -207.000000  Success rate: 0.4375 Mean loss: 0.1400\n","Epoch: 170  Cumulative reward: -206.000000  Success rate: 0.4375 Mean loss: 0.1409\n","Epoch: 175  Cumulative reward: -208.000000  Success rate: 0.3750 Mean loss: 0.1440\n","Epoch: 180  Cumulative reward: -216.000000  Success rate: 0.3125 Mean loss: 0.1457\n","Epoch: 185  Cumulative reward: -198.000000  Success rate: 0.5625 Mean loss: 0.1448\n","Epoch: 190  Cumulative reward: -203.000000  Success rate: 0.4375 Mean loss: 0.1477\n","Epoch: 195  Cumulative reward: -213.000000  Success rate: 0.3750 Mean loss: 0.1451\n","Epoch: 200  Cumulative reward: -213.000000  Success rate: 0.3750 Mean loss: 0.1526\n","Epoch: 205  Cumulative reward: -203.000000  Success rate: 0.4375 Mean loss: 0.1479\n","Epoch: 210  Cumulative reward: -204.000000  Success rate: 0.4375 Mean loss: 0.1479\n","Epoch: 215  Cumulative reward: -208.000000  Success rate: 0.4375 Mean loss: 0.1509\n","Epoch: 220  Cumulative reward: -209.000000  Success rate: 0.4375 Mean loss: 0.1629\n","Epoch: 225  Cumulative reward: -208.000000  Success rate: 0.5000 Mean loss: 0.1490\n","Epoch: 230  Cumulative reward: -204.000000  Success rate: 0.5000 Mean loss: 0.1560\n","Epoch: 235  Cumulative reward: -198.000000  Success rate: 0.5000 Mean loss: 0.1672\n","Epoch: 240  Cumulative reward: -197.000000  Success rate: 0.5625 Mean loss: 0.1609\n","Epoch: 245  Cumulative reward: -181.000000  Success rate: 0.7500 Mean loss: 0.1702\n","Epoch: 250  Cumulative reward: -195.000000  Success rate: 0.6250 Mean loss: 0.1741\n","Epoch: 255  Cumulative reward: -199.000000  Success rate: 0.5625 Mean loss: 0.1751\n","Epoch: 260  Cumulative reward: -198.000000  Success rate: 0.5625 Mean loss: 0.1715\n","Epoch: 265  Cumulative reward: -197.000000  Success rate: 0.5625 Mean loss: 0.1752\n","Epoch: 270  Cumulative reward: -176.000000  Success rate: 0.7500 Mean loss: 0.1725\n","Epoch: 275  Cumulative reward: -189.000000  Success rate: 0.7500 Mean loss: 0.1781\n","Epoch: 280  Cumulative reward: -181.000000  Success rate: 0.8125 Mean loss: 0.1834\n","Epoch: 285  Cumulative reward: -190.000000  Success rate: 0.6875 Mean loss: 0.1798\n","Epoch: 290  Cumulative reward: -183.000000  Success rate: 0.7500 Mean loss: 0.1923\n","Epoch: 295  Cumulative reward: -204.000000  Success rate: 0.5625 Mean loss: 0.1843\n","Epoch: 300  Cumulative reward: -196.000000  Success rate: 0.6250 Mean loss: 0.2008\n","Epoch: 305  Cumulative reward: -181.000000  Success rate: 0.8125 Mean loss: 0.1968\n","Epoch: 310  Cumulative reward: -183.000000  Success rate: 0.7500 Mean loss: 0.1923\n","Epoch: 315  Cumulative reward: -196.000000  Success rate: 0.6250 Mean loss: 0.1945\n","Epoch: 320  Cumulative reward: -184.000000  Success rate: 0.8125 Mean loss: 0.2025\n","Epoch: 325  Cumulative reward: -192.000000  Success rate: 0.6250 Mean loss: 0.2051\n","Epoch: 330  Cumulative reward: -192.000000  Success rate: 0.6875 Mean loss: 0.2113\n","Epoch: 335  Cumulative reward: -184.000000  Success rate: 0.7500 Mean loss: 0.2023\n","Epoch: 340  Cumulative reward: -187.000000  Success rate: 0.8125 Mean loss: 0.2029\n","Epoch: 345  Cumulative reward: -187.000000  Success rate: 0.7500 Mean loss: 0.2157\n","Epoch: 350  Cumulative reward: -172.000000  Success rate: 0.9375 Mean loss: 0.2159\n","Epoch: 355  Cumulative reward: -185.000000  Success rate: 0.8125 Mean loss: 0.2150\n","Epoch: 360  Cumulative reward: -180.000000  Success rate: 0.8750 Mean loss: 0.2130\n","Epoch: 365  Cumulative reward: -206.000000  Success rate: 0.5000 Mean loss: 0.2025\n","Epoch: 370  Cumulative reward: -185.000000  Success rate: 0.7500 Mean loss: 0.2139\n","Epoch: 375  Cumulative reward: -181.000000  Success rate: 0.8125 Mean loss: 0.2172\n","Epoch: 380  Cumulative reward: -171.000000  Success rate: 0.9375 Mean loss: 0.2265\n","Epoch: 385  Cumulative reward: -166.000000  Success rate: 0.9375 Mean loss: 0.2171\n","Epoch: 390  Cumulative reward: -195.000000  Success rate: 0.6250 Mean loss: 0.2259\n","Epoch: 395  Cumulative reward: -166.000000  Success rate: 0.9375 Mean loss: 0.2268\n","Epoch: 400  Cumulative reward: -182.000000  Success rate: 0.8125 Mean loss: 0.2361\n","Epoch: 405  Cumulative reward: -184.000000  Success rate: 0.8125 Mean loss: 0.2333\n","Epoch: 410  Cumulative reward: -172.000000  Success rate: 0.8750 Mean loss: 0.2352\n","Epoch: 415  Cumulative reward: -178.000000  Success rate: 0.8125 Mean loss: 0.2368\n","Epoch: 420  Cumulative reward: -179.000000  Success rate: 0.8750 Mean loss: 0.2482\n","Epoch: 425  Cumulative reward: -164.000000  Success rate: 0.8750 Mean loss: 0.2373\n","Epoch: 430  Cumulative reward: -176.000000  Success rate: 0.9375 Mean loss: 0.2420\n","Epoch: 435  Cumulative reward: -191.000000  Success rate: 0.6875 Mean loss: 0.2379\n","Epoch: 440  Cumulative reward: -175.000000  Success rate: 0.9375 Mean loss: 0.2531\n","Epoch: 445  Cumulative reward: -202.000000  Success rate: 0.5625 Mean loss: 0.2520\n","Epoch: 450  Cumulative reward: -180.000000  Success rate: 0.8750 Mean loss: 0.2488\n","Epoch: 455  Cumulative reward: -175.000000  Success rate: 0.8750 Mean loss: 0.2604\n","Epoch: 460  Cumulative reward: -185.000000  Success rate: 0.7500 Mean loss: 0.2514\n","Epoch: 465  Cumulative reward: -178.000000  Success rate: 0.8750 Mean loss: 0.2565\n","Epoch: 470  Cumulative reward: -174.000000  Success rate: 0.9375 Mean loss: 0.2576\n","Epoch: 475  Cumulative reward: -175.000000  Success rate: 0.8750 Mean loss: 0.2435\n","Epoch: 480  Cumulative reward: -173.000000  Success rate: 0.8750 Mean loss: 0.2504\n","Epoch: 485  Cumulative reward: -185.000000  Success rate: 0.8125 Mean loss: 0.2602\n","Epoch: 490  Cumulative reward: -174.000000  Success rate: 0.8750 Mean loss: 0.2577\n","Epoch: 495  Cumulative reward: -185.000000  Success rate: 0.8125 Mean loss: 0.2509\n","Running bit flip environment with 15 bits and HER policy: random\n","Epoch: 0  Cumulative reward: -240.000000  Success rate: 0.0000 Mean loss: 0.1266\n","Epoch: 5  Cumulative reward: -240.000000  Success rate: 0.0000 Mean loss: 0.0381\n","Epoch: 10  Cumulative reward: -240.000000  Success rate: 0.0000 Mean loss: 0.0521\n","Epoch: 15  Cumulative reward: -240.000000  Success rate: 0.0000 Mean loss: 0.0532\n","Epoch: 20  Cumulative reward: -240.000000  Success rate: 0.0000 Mean loss: 0.0548\n","Epoch: 25  Cumulative reward: -240.000000  Success rate: 0.0000 Mean loss: 0.0538\n","Epoch: 30  Cumulative reward: -240.000000  Success rate: 0.0000 Mean loss: 0.0544\n","Epoch: 35  Cumulative reward: -240.000000  Success rate: 0.0000 Mean loss: 0.0526\n","Epoch: 40  Cumulative reward: -240.000000  Success rate: 0.0000 Mean loss: 0.0537\n","Epoch: 45  Cumulative reward: -240.000000  Success rate: 0.0000 Mean loss: 0.0555\n","Epoch: 50  Cumulative reward: -240.000000  Success rate: 0.0000 Mean loss: 0.0569\n","Epoch: 55  Cumulative reward: -240.000000  Success rate: 0.0000 Mean loss: 0.0576\n","Epoch: 60  Cumulative reward: -240.000000  Success rate: 0.0000 Mean loss: 0.0498\n","Epoch: 65  Cumulative reward: -240.000000  Success rate: 0.0000 Mean loss: 0.0516\n","Epoch: 70  Cumulative reward: -240.000000  Success rate: 0.0000 Mean loss: 0.0533\n","Epoch: 75  Cumulative reward: -240.000000  Success rate: 0.0000 Mean loss: 0.0552\n","Epoch: 80  Cumulative reward: -240.000000  Success rate: 0.0000 Mean loss: 0.0513\n","Epoch: 85  Cumulative reward: -240.000000  Success rate: 0.0000 Mean loss: 0.0541\n","Epoch: 90  Cumulative reward: -240.000000  Success rate: 0.0000 Mean loss: 0.0582\n","Epoch: 95  Cumulative reward: -240.000000  Success rate: 0.0000 Mean loss: 0.0561\n","Epoch: 100  Cumulative reward: -240.000000  Success rate: 0.0000 Mean loss: 0.0562\n","Epoch: 105  Cumulative reward: -240.000000  Success rate: 0.0000 Mean loss: 0.0509\n","Epoch: 110  Cumulative reward: -240.000000  Success rate: 0.0000 Mean loss: 0.0578\n","Epoch: 115  Cumulative reward: -240.000000  Success rate: 0.0000 Mean loss: 0.0594\n","Epoch: 120  Cumulative reward: -240.000000  Success rate: 0.0000 Mean loss: 0.0518\n","Epoch: 125  Cumulative reward: -240.000000  Success rate: 0.0000 Mean loss: 0.0526\n","Epoch: 130  Cumulative reward: -240.000000  Success rate: 0.0000 Mean loss: 0.0545\n","Epoch: 135  Cumulative reward: -239.000000  Success rate: 0.0625 Mean loss: 0.0565\n","Epoch: 140  Cumulative reward: -240.000000  Success rate: 0.0000 Mean loss: 0.0523\n","Epoch: 145  Cumulative reward: -240.000000  Success rate: 0.0000 Mean loss: 0.0535\n","Epoch: 150  Cumulative reward: -240.000000  Success rate: 0.0000 Mean loss: 0.0550\n","Epoch: 155  Cumulative reward: -240.000000  Success rate: 0.0000 Mean loss: 0.0533\n","Epoch: 160  Cumulative reward: -240.000000  Success rate: 0.0000 Mean loss: 0.0580\n","Epoch: 165  Cumulative reward: -240.000000  Success rate: 0.0000 Mean loss: 0.0543\n","Epoch: 170  Cumulative reward: -240.000000  Success rate: 0.0000 Mean loss: 0.0567\n","Epoch: 175  Cumulative reward: -240.000000  Success rate: 0.0000 Mean loss: 0.0531\n","Epoch: 180  Cumulative reward: -235.000000  Success rate: 0.0625 Mean loss: 0.0621\n","Epoch: 185  Cumulative reward: -235.000000  Success rate: 0.0625 Mean loss: 0.0592\n","Epoch: 190  Cumulative reward: -240.000000  Success rate: 0.0000 Mean loss: 0.0620\n","Epoch: 195  Cumulative reward: -240.000000  Success rate: 0.0000 Mean loss: 0.0529\n","Epoch: 200  Cumulative reward: -240.000000  Success rate: 0.0000 Mean loss: 0.0587\n","Epoch: 205  Cumulative reward: -240.000000  Success rate: 0.0000 Mean loss: 0.0530\n","Epoch: 210  Cumulative reward: -240.000000  Success rate: 0.0000 Mean loss: 0.0559\n","Epoch: 215  Cumulative reward: -240.000000  Success rate: 0.0000 Mean loss: 0.0543\n","Epoch: 220  Cumulative reward: -240.000000  Success rate: 0.0000 Mean loss: 0.0559\n","Epoch: 225  Cumulative reward: -240.000000  Success rate: 0.0000 Mean loss: 0.0582\n","Epoch: 230  Cumulative reward: -235.000000  Success rate: 0.0625 Mean loss: 0.0627\n","Epoch: 235  Cumulative reward: -240.000000  Success rate: 0.0000 Mean loss: 0.0564\n","Epoch: 240  Cumulative reward: -240.000000  Success rate: 0.0000 Mean loss: 0.0567\n","Epoch: 245  Cumulative reward: -236.000000  Success rate: 0.0625 Mean loss: 0.0599\n","Epoch: 250  Cumulative reward: -240.000000  Success rate: 0.0000 Mean loss: 0.0562\n","Epoch: 255  Cumulative reward: -240.000000  Success rate: 0.0000 Mean loss: 0.0515\n","Epoch: 260  Cumulative reward: -239.000000  Success rate: 0.0625 Mean loss: 0.0583\n","Epoch: 265  Cumulative reward: -240.000000  Success rate: 0.0000 Mean loss: 0.0636\n","Epoch: 270  Cumulative reward: -240.000000  Success rate: 0.0000 Mean loss: 0.0629\n","Epoch: 275  Cumulative reward: -240.000000  Success rate: 0.0000 Mean loss: 0.0596\n","Epoch: 280  Cumulative reward: -240.000000  Success rate: 0.0000 Mean loss: 0.0571\n","Epoch: 285  Cumulative reward: -240.000000  Success rate: 0.0000 Mean loss: 0.0567\n","Epoch: 290  Cumulative reward: -236.000000  Success rate: 0.0625 Mean loss: 0.0583\n","Epoch: 295  Cumulative reward: -233.000000  Success rate: 0.0625 Mean loss: 0.0602\n","Epoch: 300  Cumulative reward: -240.000000  Success rate: 0.0000 Mean loss: 0.0527\n","Epoch: 305  Cumulative reward: -240.000000  Success rate: 0.0000 Mean loss: 0.0573\n","Epoch: 310  Cumulative reward: -229.000000  Success rate: 0.1250 Mean loss: 0.0600\n","Epoch: 315  Cumulative reward: -234.000000  Success rate: 0.0625 Mean loss: 0.0585\n","Epoch: 320  Cumulative reward: -240.000000  Success rate: 0.0000 Mean loss: 0.0576\n","Epoch: 325  Cumulative reward: -234.000000  Success rate: 0.0625 Mean loss: 0.0584\n","Epoch: 330  Cumulative reward: -240.000000  Success rate: 0.0000 Mean loss: 0.0670\n","Epoch: 335  Cumulative reward: -240.000000  Success rate: 0.0000 Mean loss: 0.0654\n","Epoch: 340  Cumulative reward: -240.000000  Success rate: 0.0000 Mean loss: 0.0648\n","Epoch: 345  Cumulative reward: -240.000000  Success rate: 0.0000 Mean loss: 0.0562\n","Epoch: 350  Cumulative reward: -236.000000  Success rate: 0.0625 Mean loss: 0.0620\n","Epoch: 355  Cumulative reward: -240.000000  Success rate: 0.0000 Mean loss: 0.0680\n","Epoch: 360  Cumulative reward: -240.000000  Success rate: 0.0000 Mean loss: 0.0638\n","Epoch: 365  Cumulative reward: -240.000000  Success rate: 0.0000 Mean loss: 0.0662\n","Epoch: 370  Cumulative reward: -235.000000  Success rate: 0.0625 Mean loss: 0.0631\n","Epoch: 375  Cumulative reward: -240.000000  Success rate: 0.0000 Mean loss: 0.0607\n","Epoch: 380  Cumulative reward: -240.000000  Success rate: 0.0000 Mean loss: 0.0579\n","Epoch: 385  Cumulative reward: -240.000000  Success rate: 0.0000 Mean loss: 0.0581\n","Epoch: 390  Cumulative reward: -234.000000  Success rate: 0.0625 Mean loss: 0.0606\n","Epoch: 395  Cumulative reward: -240.000000  Success rate: 0.0000 Mean loss: 0.0589\n","Epoch: 400  Cumulative reward: -235.000000  Success rate: 0.0625 Mean loss: 0.0632\n","Epoch: 405  Cumulative reward: -240.000000  Success rate: 0.0000 Mean loss: 0.0652\n","Epoch: 410  Cumulative reward: -240.000000  Success rate: 0.0000 Mean loss: 0.0663\n","Epoch: 415  Cumulative reward: -240.000000  Success rate: 0.0000 Mean loss: 0.0617\n","Epoch: 420  Cumulative reward: -240.000000  Success rate: 0.0000 Mean loss: 0.0605\n","Epoch: 425  Cumulative reward: -240.000000  Success rate: 0.0000 Mean loss: 0.0648\n","Epoch: 430  Cumulative reward: -240.000000  Success rate: 0.0000 Mean loss: 0.0614\n","Epoch: 435  Cumulative reward: -233.000000  Success rate: 0.0625 Mean loss: 0.0638\n","Epoch: 440  Cumulative reward: -240.000000  Success rate: 0.0000 Mean loss: 0.0643\n","Epoch: 445  Cumulative reward: -240.000000  Success rate: 0.0000 Mean loss: 0.0653\n","Epoch: 450  Cumulative reward: -240.000000  Success rate: 0.0000 Mean loss: 0.0636\n","Epoch: 455  Cumulative reward: -235.000000  Success rate: 0.0625 Mean loss: 0.0635\n","Epoch: 460  Cumulative reward: -240.000000  Success rate: 0.0000 Mean loss: 0.0647\n","Epoch: 465  Cumulative reward: -240.000000  Success rate: 0.0000 Mean loss: 0.0653\n","Epoch: 470  Cumulative reward: -240.000000  Success rate: 0.0000 Mean loss: 0.0617\n","Epoch: 475  Cumulative reward: -231.000000  Success rate: 0.1250 Mean loss: 0.0667\n","Epoch: 480  Cumulative reward: -231.000000  Success rate: 0.1250 Mean loss: 0.0617\n","Epoch: 485  Cumulative reward: -235.000000  Success rate: 0.0625 Mean loss: 0.0624\n","Epoch: 490  Cumulative reward: -233.000000  Success rate: 0.0625 Mean loss: 0.0641\n","Epoch: 495  Cumulative reward: -224.000000  Success rate: 0.1875 Mean loss: 0.0676\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"2He1zYQDUNJ7"},"source":["Explanations needed.. (in submission file)"]},{"cell_type":"markdown","metadata":{"id":"lua6jJR-cnv2"},"source":["# Sawyer Environment Goal-Conditioned RL"]},{"cell_type":"code","metadata":{"id":"7ojAJB_4colS"},"source":["#@title Buffer\n","#@markdown Same as the Buffer class before but placed here in the event you want to run the sections separately\n","import numpy as np\n","import random\n","from collections import deque \n","\n","class Buffer(object) :\n","\n","\tdef __init__(self,size,sample_size):\n","\n","\t\tself.size = size\n","\t\tself.sample_size = sample_size\n","\t\tself.buffer = deque()\n","\n","\tdef add(self,state,action,reward,next_state) :\n","\t\tself.buffer.append((state,action,reward,next_state))\n","\n","\t\tif len(self.buffer) > self.size:\n","\t\t\tself.buffer.popleft()\n","\n","\tdef sample(self) :\n","\t\tif len(self.buffer) < self.sample_size:\n","\t\t\tsamples = self.buffer\n","\t\telse:\t\n","\t\t\tsamples = random.sample(self.buffer,self.sample_size)\n","\t\t\n","\t\tstate = np.reshape(np.array([arr[0] for arr in samples]),[len(samples),-1])\n","\t\taction = np.array([arr[1] for arr in samples])\n","\t\treward = np.array([arr[2] for arr in samples])\n","\t\tnext_state = np.reshape(np.array([arr[3] for arr in samples]),[len(samples),-1])\n","\n","\t\treturn state, action, reward, next_state\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"V4b_b31-lQFp","executionInfo":{"status":"ok","timestamp":1603647374637,"user_tz":420,"elapsed":11530,"user":{"displayName":"Yiheng Li","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ggo9FSuWqtcs815uMwgcd6czNNqCSqb_lRojxQg=s64","userId":"15227785711110309391"}},"outputId":"0269f83a-ef97-4ab2-956b-e6ea56d75084","colab":{"base_uri":"https://localhost:8080/","height":51}},"source":["import numpy as np\n","import tensorflow as tf\n","from matplotlib import pyplot as plt\n","import multiworld\n","import glfw\n","\n","multiworld.register_all_envs()   \n","\n","class Model(tf.keras.Model):\n","\n","  def __init__(self, num_act):\n","    super(Model, self).__init__()\n","\n","    hidden_dim = 256\n","    self.dense1 = tf.keras.layers.Dense(hidden_dim, activation=tf.nn.relu)\n","    self.out = tf.keras.layers.Dense(num_act,activation = None)\n","\n","  def call(self, inputs):\n","    x = self.dense1(inputs)\n","    return self.out(x)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["pygame 1.9.6\n","Hello from the pygame community. https://www.pygame.org/contribute.html\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"AVtTlZFSlSi2"},"source":["# ************   Helper functions    ************ #\n","# Globals\n","\n","NUM_DIM = 2\n","NUM_ACT = 4\n","done_threshold = -0.01\n","Sawyer_Env = env = wrap_env(gym.make('SawyerReachXYEnv-v1'))\n","\n","def updateTarget(model, target_model, tau=0.95) :\n","    model_weights = model.get_weights()\n","    target_weights = target_model.get_weights()\n","    new_weights = []\n","    for i, weight in enumerate(model_weights):\n","      new_weights.append(tau * target_weights[i] + (1 - tau) * weight)\n","\n","    target_model.set_weights(new_weights)\n","\n","def take_action(action, render):\n","    '''passes the discrete action selected by the Q-network to the Sawyer Arm.\n","    The function returns the next state, the reward, and whether the environment\n","    was solved. The environment done returned is not the same as the environment\n","    done returned by the Sawyer environment. Due to discretization, it may not be\n","    possible to exactly reach the goal. The done flag returns true if the end\n","    state is within done_threshold of the final goal\n","\n","    inputs:  action - integer (0 to NUM_ACT-1) selected by the Q-network\n","    outputs: next_state - new state (x, y) location of arm\n","             reward - reward returned by Sawyer environment\n","             done - boolean whether environment is solved'''\n","\n","    # maps actions selected by Q-network to Sawyer arm actions\n","    # array MUST be length NUM_ACT\n","    action_dic = {0:[-1, 0], 1:[1, 0], 2:[0, -1], 3:[0, 1]}\n","    # look up which action in Sawyer arm space corresponds to the selected integer action\n","    action_sawyer = np.array(action_dic[action], dtype=np.float32)\n","    # take the action\n","    ob, reward, done, info = Sawyer_Env.step(action_sawyer)\n","    # if rendering is turned on, render the environment\n","    if render:\n","        Sawyer_Env.render(mode='rgb_array')\n","    # check if we're \"close enough\" to declare done\n","    if reward > done_threshold:\n","        done = True\n","\n","    # pull the observed state off\n","    next_state = ob['observation'][0:2]\n","\n","    return next_state, reward, done, info\n","\n","def solve_environment(model, state, goal_state, total_reward, steps_per_episode, render):\n","    '''attempt to solve the Sawyer Arm environment using the current policy'''\n","    \n","    # list for recording what happened in the episode\n","    episode_experience = []\n","    succeeded = False\n","\n","    for t in range(steps_per_episode):\n","      # attempt to solve the state - number of steps given to solve the\n","      # state is equal to the passed argument steps_per_episode.\n","\n","      # ======================== TODO modify code ========================\n","      inputs = np.concatenate([state, goal_state])\n","      inputs = np.expand_dims(inputs, axis=0)\n","      inputs = np.array(inputs, dtype=np.float32)\n","      # forward pass to find action\n","      out = model(inputs)\n","      action = np.argmax(out, axis=1)[0]\n","      # take the action - use helper function to convert discrete actions to\n","      next_state,reward,done, _ = take_action(action, render)\n","      # actions in the Sawyer environment\n","\n","      # add to the episode experience (what happened)\n","      episode_experience.append((state,action,reward,next_state,goal_state))\n","      # calculate total reward\n","      total_reward += reward\n","      # update state\n","      state = next_state\n","      # mark that we've finished the episode and succeeded with training\n","      if done:\n","          if succeeded:\n","              continue\n","          else:\n","              succeeded = True\n","      # ========================      END TODO       ========================\n","\n","\n","    return succeeded, episode_experience, total_reward\n","\n","\n","def solve_environment_no_goal(model, state, goal_state, total_reward, steps_per_episode, render):\n","    '''attempt to solve the Sawyer Arm environment using the current policy with no goal condition'''\n","    \n","    # list for recording what happened in the episode\n","    episode_experience = []\n","    succeeded = False\n","\n","    for t in range(steps_per_episode):\n","        inputs = state\n","        inputs = np.expand_dims(inputs, axis=0)\n","        inputs = np.array(inputs, dtype=np.float32) \n","        # forward pass to find action\n","        out = model(inputs)\n","        action = np.argmax(out,axis = 1)[0]\n","        next_state,reward,done, _ = take_action(action, render)\n","        # add to the episode experience (what happened)\n","        episode_experience.append((state,action,reward,next_state,goal_state))\n","        # calculate total reward\n","        total_reward += reward\n","        # update state\n","        state = next_state\n","        # mark that we've finished the episode and succeeded with training\n","        if done:\n","            if succeeded:\n","                continue\n","            else:\n","                succeeded = True\n","    else:\n","         env.stats_recorder.save_complete()\n","         env.stats_recorder.done = True\n","\n","    return succeeded, episode_experience, total_reward\n","\n","def update_replay_buffer(steps_per_episode, num_relabeled, replay_buffer, episode_experience, HER):\n","    '''adds past experience to the replay buffer. Training is done with episodes from the replay\n","    buffer. When HER is used, num_relabeled additional relabeled data points are also added\n","    to the replay buffer\n","\n","    inputs:    epsidode_experience - list of transitions from the last episode\n","    modifies:  replay_buffer\n","    outputs:   None'''\n","\n","    for t in range(steps_per_episode) :\n","        # copy actual experience from episode_experience to replay_buffer\n","\n","        # ======================== TODO modify code ========================\n","        s,a,r,s_,g = episode_experience[t]\n","        # state\n","        inputs = np.concatenate([s,g])\n","        # next state\n","        inputs_ = np.concatenate([s_,g])\n","        # add to the replay buffer\n","        replay_buffer.add(inputs,a,r,inputs_)\n","\n","        # when HER is used, each call to update_replay_buffer should add num_relabeled\n","        # relabeled points to the replay buffer per step\n","        if HER == 'None':\n","            # HER not being used, so do nothing\n","            pass\n","\n","        elif HER == 'final':\n","            # final - relabel based on final state in episode\n","            _,_,_,final_state,_ = episode_experience[steps_per_episode-1]\n","            # recalculate r\n","            r = np.linalg.norm(s - final_state)\n","            replay_buffer.add(np.concatenate([s,final_state]),a,r,np.concatenate([s_,final_state]))\n","\n","        elif HER == 'future':\n","            # future - relabel based on future state. At each timestep t, relabel the\n","            # goal with a randomly select timestep between t and the end of the\n","            # episode\n","            for i in range(num_relabeled):\n","              future_time = random.randint(t,steps_per_episode-1)\n","              _,_,_,future_state,_ = episode_experience[future_time]\n","              # recalculate r\n","              r = np.linalg.norm(s - future_state)\n","              replay_buffer.add(np.concatenate([s, future_state]),a,r,np.concatenate([s_, future_state]))\n","\n","        elif HER == 'random':\n","            # random - relabel based on a random state in the episode\n","            for i in range(num_relabeled):\n","              random_time = random.randint(0, steps_per_episode-1)\n","              _,_,_,random_state,_ = episode_experience[random_time]\n","              # recalculate r\n","              r = np.linalg.norm(s - random_state)\n","              replay_buffer.add(np.concatenate([s, random_state]),a,r,np.concatenate([s_,random_state]))\n","\n","\n","\n","        # ========================      END TODO       ========================\n","\n","        else:\n","            print(\"Invalid value for Her flag - HER not used\")\n","    return\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"xX0tbP_w9ViT"},"source":["# ************   Main Training Loop    ************ #\n","\n","def run_sawyer(num_epochs, buffer_size = 1e6, batch_size = 128, \n","               num_episodes = 16, num_relabeled = 4, gamma = 0.98, log_interval=5, opt_steps=40,\n","               steps_per_episode=50, render=False, HER = \"None\"):\n","    '''Main loop for running in the Sawyer environment. The DQN is\n","    trained over num_epochs. In each epoch, the agent runs in the environment\n","    num_episodes number of times. The Q-target and Q-policy networks are\n","    updated at the end of each epoch. Within one episode, Q-policy attempts\n","    to solve the environment and is limited to the same number as steps as the\n","    size of the environment\n","\n","    inputs: HER - string specifying whether to use HER'''\n","    # create Sawyer arm environment and replay buffer\n","    replay_buffer = Buffer(buffer_size,batch_size)\n","\n","    # set up Q-policy (model) and Q-target (target_model)\n","    model = Model(NUM_ACT)\n","    target_model = Model(NUM_ACT)\n","\n","    optimizer = tf.keras.optimizers.Adam(learning_rate = 1e-3)\n","\n","    # ======================== TODO modify code ========================\n","    # modify to be goal conditioned\n","    reset_state = Sawyer_Env.reset()  \n","    state = reset_state['observation'][:2]          # look up the state\n","    goal_state = reset_state['desired_goal'][:2]    # look up the goal\n","    inputs = np.concatenate([state, goal_state])\n","    inputs = np.expand_dims(inputs, axis=0)  \n","    model(inputs)\n","    target_model(inputs)\n","\n","    # start by making Q-target and Q-policy the same\n","    updateTarget(model, target_model, tau=0.0)\n","\n","    # ========================      END TODO       ========================\n","\n","    total_loss = []                  # training loss for each epoch\n","    success_rate = []                # success rate for each epoch\n","    \n","    for i in range(num_epochs):\n","        # Run for a fixed number of epochs\n","\n","        total_reward = 0.0           # total reward for the epoch\n","        successes = []               # record success rate for each episode of the epoch\n","        losses = []                  # loss at the end of each epoch\n","\n","        for k in range(num_episodes):\n","            reset_state = Sawyer_Env.reset()                # reset the environment\n","            state = reset_state['observation'][:2]          # look up the state\n","            goal_state = reset_state['desired_goal'][:2]    # look up the goal\n","\n","            # attempt to solve the environment\n","            # ======================== TODO modify code ========================\n","            # modify to be goal conditioned\n","            succeeded, episode_experience, total_reward = solve_environment(model, state, goal_state, total_reward, steps_per_episode, render)\n","            # ========================      END TODO       ========================\n","\n","            successes.append(succeeded)                     # track whether we succeeded in environment \n","            update_replay_buffer(steps_per_episode, num_relabeled, replay_buffer, episode_experience, HER)   # add to the replay buffer; use specified  HER policy\n","            env.close() \n","            glfw.terminate()\n","        for k in range(opt_steps):\n","            # optimize the Q-policy network\n","\n","            # sample from the replay buffer\n","            state,action,reward,next_state = replay_buffer.sample()\n","            state = np.array(state, dtype=np.float32) \n","            next_state = np.array(next_state, dtype=np.float32) \n","            # forward pass through target network   \n","\n","            with tf.GradientTape() as tape:\n","              target_net_Q = target_model(next_state)\n","              # calculate target reward\n","              target_reward = np.clip(np.reshape(reward,[-1]) + gamma * np.reshape(np.max(target_net_Q,axis = -1),[-1]),-1. / (1 - gamma), 0)\n","              # calculate loss\n","              model_predict = model(state)\n","              model_action_taken = np.reshape(action,[-1])\n","              action_one_hot = tf.one_hot(model_action_taken, NUM_ACT)\n","              Q_val = tf.reduce_sum(model_predict * action_one_hot, axis=1)\n","              loss = tf.reduce_mean(tf.square(Q_val - target_reward))\n","              losses.append(loss)\n","            \n","            gradients = tape.gradient(loss, model.trainable_variables)\n","            optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n","            \n","        updateTarget(model, target_model)               # update target model by copying Q-policy to Q-target      \n","        success_rate.append(np.mean(successes))       # append mean success rate for this epoch\n","\n","        if i % log_interval == 0:\n","            print('Epoch: %d  Cumulative reward: %f  Success rate: %.4f Mean loss: %.4f' % (i, total_reward, np.mean(successes), np.mean(losses)))\n","    return success_rate"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"IMNnVQsNAgni","executionInfo":{"status":"ok","timestamp":1603649106942,"user_tz":420,"elapsed":506580,"user":{"displayName":"Yiheng Li","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ggo9FSuWqtcs815uMwgcd6czNNqCSqb_lRojxQg=s64","userId":"15227785711110309391"}},"outputId":"c960c4d0-5295-4ed3-bb8f-42f9db369d3d","colab":{"base_uri":"https://localhost:8080/","height":751}},"source":["success_rate9 = run_sawyer(num_epochs=150, HER = \"None\")\n","success_rate10 = run_sawyer(num_epochs=150, HER = \"final\")"],"execution_count":null,"outputs":[{"output_type":"stream","text":["WARNING:tensorflow:Layer model_4 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n","\n","If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n","\n","To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n","\n","WARNING:tensorflow:Layer model_5 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n","\n","If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n","\n","To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n","\n","Epoch: 0  Cumulative reward: -135.449593  Success rate: 0.0000 Mean loss: 0.0021\n","Epoch: 5  Cumulative reward: -86.159183  Success rate: 0.0625 Mean loss: 0.0009\n","Epoch: 10  Cumulative reward: -28.733250  Success rate: 0.3750 Mean loss: 0.0004\n","Epoch: 15  Cumulative reward: -25.710331  Success rate: 0.7500 Mean loss: 0.0003\n","Epoch: 20  Cumulative reward: -29.467537  Success rate: 0.5000 Mean loss: 0.0004\n","Epoch: 25  Cumulative reward: -24.519974  Success rate: 0.6875 Mean loss: 0.0004\n","Epoch: 30  Cumulative reward: -26.113723  Success rate: 0.5625 Mean loss: 0.0005\n","Epoch: 35  Cumulative reward: -22.903232  Success rate: 0.6250 Mean loss: 0.0006\n","Epoch: 40  Cumulative reward: -24.280279  Success rate: 0.5000 Mean loss: 0.0006\n","Epoch: 45  Cumulative reward: -20.784860  Success rate: 0.7500 Mean loss: 0.0006\n","Epoch: 50  Cumulative reward: -25.520676  Success rate: 0.3750 Mean loss: 0.0006\n","Epoch: 55  Cumulative reward: -19.256821  Success rate: 0.8750 Mean loss: 0.0006\n","Epoch: 60  Cumulative reward: -21.026684  Success rate: 0.6875 Mean loss: 0.0005\n","Epoch: 65  Cumulative reward: -27.579661  Success rate: 0.6250 Mean loss: 0.0006\n","Epoch: 70  Cumulative reward: -26.631221  Success rate: 0.1250 Mean loss: 0.0006\n","Epoch: 75  Cumulative reward: -23.922696  Success rate: 0.8750 Mean loss: 0.0006\n","Epoch: 80  Cumulative reward: -23.505290  Success rate: 0.3750 Mean loss: 0.0006\n","Epoch: 85  Cumulative reward: -26.417304  Success rate: 0.6250 Mean loss: 0.0007\n","Epoch: 90  Cumulative reward: -23.635948  Success rate: 0.7500 Mean loss: 0.0005\n","Epoch: 95  Cumulative reward: -24.364655  Success rate: 0.6250 Mean loss: 0.0007\n","Epoch: 100  Cumulative reward: -27.966893  Success rate: 0.3750 Mean loss: 0.0007\n","Epoch: 105  Cumulative reward: -34.407263  Success rate: 0.1250 Mean loss: 0.0006\n","Epoch: 110  Cumulative reward: -32.351303  Success rate: 0.3125 Mean loss: 0.0007\n","Epoch: 115  Cumulative reward: -22.552579  Success rate: 0.6875 Mean loss: 0.0005\n","Epoch: 120  Cumulative reward: -24.618688  Success rate: 0.3750 Mean loss: 0.0006\n","Epoch: 125  Cumulative reward: -25.664109  Success rate: 0.1875 Mean loss: 0.0006\n","Epoch: 130  Cumulative reward: -33.206878  Success rate: 0.2500 Mean loss: 0.0006\n","Epoch: 135  Cumulative reward: -41.047290  Success rate: 0.0625 Mean loss: 0.0006\n","Epoch: 140  Cumulative reward: -23.415106  Success rate: 0.8125 Mean loss: 0.0006\n","Epoch: 145  Cumulative reward: -24.213834  Success rate: 0.6875 Mean loss: 0.0006\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Sbhq3UiESmon"},"source":["# If you chose to render:\n","show_video()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"2UKXENISPtf5"},"source":["# Plotting Code\n","\n","We've provided some sample plotting code for you. Feel free to customize it per the assignment specifications. The code will not be graded."]},{"cell_type":"code","metadata":{"id":"NOzYeO5EAo6c","executionInfo":{"status":"ok","timestamp":1603649243607,"user_tz":420,"elapsed":2496,"user":{"displayName":"Yiheng Li","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ggo9FSuWqtcs815uMwgcd6czNNqCSqb_lRojxQg=s64","userId":"15227785711110309391"}},"outputId":"ee4bfc96-727e-426e-ece2-0c7a8c560e4c","colab":{"base_uri":"https://localhost:8080/","height":68}},"source":["pip install plotly"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: plotly in /usr/local/lib/python3.6/dist-packages (4.4.1)\n","Requirement already satisfied: retrying>=1.3.3 in /usr/local/lib/python3.6/dist-packages (from plotly) (1.3.3)\n","Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from plotly) (1.15.0)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"OrP-V9ZJHW-3","executionInfo":{"status":"ok","timestamp":1603649311573,"user_tz":420,"elapsed":2225,"user":{"displayName":"Yiheng Li","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ggo9FSuWqtcs815uMwgcd6czNNqCSqb_lRojxQg=s64","userId":"15227785711110309391"}},"outputId":"6f42ff3a-e5ed-427e-a008-17cf3292d357","colab":{"base_uri":"https://localhost:8080/","height":467,"output_embedded_package_id":"11d94DxmH109aFIYch6M1hGj2aCH3pTs9"}},"source":["from IPython.display import HTML\n","from plotly import graph_objs as go\n","\n","# Sample plotting clode (replace successes where necessary)\n","exp_to_accuracies = {\n","    \"Sawyer_her_none\": success_rate9,\n","    \"Sawyer_her_final\": success_rate10,\n","}\n","\n","# Creates the Figure\n","fig = go.Figure()\n","data = []\n","for experiment, accuracies in exp_to_accuracies.items():\n","  steps = range(len(accuracies))\n","  steps = [5 * x for x in steps]\n","  data.append(go.Scatter(x=steps, y=accuracies, line_shape='spline', name=experiment))\n","\n","# Applies a custom layout\n","layout = go.Layout(\n","    title=go.layout.Title(\n","        text='Sawyer',\n","        x=0.5\n","    ),\n","    xaxis=go.layout.XAxis(\n","        title=go.layout.xaxis.Title(\n","            text='Epoch',\n","            font=dict(\n","                family='Courier New, monospace',\n","                size=18,\n","                color='#7f7f7f'\n","            )\n","        )\n","    ),\n","    yaxis=go.layout.YAxis(\n","        title=go.layout.yaxis.Title(\n","            text='Success Rate',\n","            font=dict(\n","                family='Courier New, monospace',\n","                size=18,\n","                color='#7f7f7f'\n","            )\n","        )\n","    )\n",")\n","\n","fig = go.Figure(data=data, layout=layout)\n","\n","HTML(fig.to_html())"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}]},{"cell_type":"code","metadata":{"id":"GqMV9wr0Jdn9","executionInfo":{"status":"ok","timestamp":1603600602144,"user_tz":420,"elapsed":282,"user":{"displayName":"Yiheng Li","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ggo9FSuWqtcs815uMwgcd6czNNqCSqb_lRojxQg=s64","userId":"15227785711110309391"}},"outputId":"477b3c8f-bd77-4d99-e819-819f07f8574a","colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["len(success_rate2)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["150"]},"metadata":{"tags":[]},"execution_count":27}]},{"cell_type":"code","metadata":{"id":"yTU7Yv3kCXim"},"source":[""],"execution_count":null,"outputs":[]}]}