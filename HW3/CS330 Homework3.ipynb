{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"CS330 Homework3.ipynb","provenance":[{"file_id":"1uYqzq4Ix91DD4QdElyrbC_If0TKDf_3t","timestamp":1603241542330},{"file_id":"1VDBw9P1Bs1Xpzp8B3mxmQAdcXrh15yOZ","timestamp":1602965367864}],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"fwmQwFeQo2qW"},"source":["## CS 330 Homework 3 Installation\n","\n","The following code blocks will install the required libraries.\n"]},{"cell_type":"markdown","metadata":{"id":"EB7xAlJMrHwA"},"source":["## Setup for Google Drive and Required Libraries\n"]},{"cell_type":"code","metadata":{"id":"pa0Ri_edrNIT","cellView":"both","executionInfo":{"status":"ok","timestamp":1603385050418,"user_tz":420,"elapsed":339,"user":{"displayName":"Yiheng Li","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ggo9FSuWqtcs815uMwgcd6czNNqCSqb_lRojxQg=s64","userId":"15227785711110309391"}},"outputId":"302c274c-f3e0-4cbf-e196-7495ae94e7fc","colab":{"base_uri":"https://localhost:8080/","height":54}},"source":["#@title Mount Google Drive\n","#@markdown Your work will be stored in a folder called `cs330_fall2020` by default to prevent Colab instance timeouts \n","#@markdown from deleting your edits and requiring you to redownload the mujoco library. Feel free to use this if you want to write out plots.\n","\n","import os\n","from google.colab import drive\n","drive.mount('/content/gdrive')\n","\n","#@title set up mount symlink\n","\n","DRIVE_PATH = '/content/gdrive/My\\ Drive/cs330_fall2020'\n","DRIVE_PYTHON_PATH = DRIVE_PATH.replace('\\\\', '')\n","if not os.path.exists(DRIVE_PYTHON_PATH):\n","  %mkdir $DRIVE_PATH\n","\n","## the space in `My Drive` causes some issues,\n","## make a symlink to avoid this\n","SYM_PATH = '/content/cs330_fall2020'\n","if not os.path.exists(SYM_PATH):\n","  !ln -s $DRIVE_PATH $SYM_PATH"],"execution_count":8,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"f-raFHMIpUun","executionInfo":{"status":"ok","timestamp":1603385058799,"user_tz":420,"elapsed":6964,"user":{"displayName":"Yiheng Li","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ggo9FSuWqtcs815uMwgcd6czNNqCSqb_lRojxQg=s64","userId":"15227785711110309391"}},"outputId":"4bac2f63-dc21-4656-e92c-673e8e007061","colab":{"base_uri":"https://localhost:8080/","height":646}},"source":["#@title Install Requirements\n","#@markdown Requirements for the assignment and display drivers\n","\n","# Robot sim\n","!pip install gym==0.15.4\n","!pip install pygame\n","\n","# Various things for render\n","!apt-get install python-opengl -y\n","!apt install xvfb -y\n","\n","# Rendering Environment\n","!pip install pyvirtualdisplay\n","!pip install piglet\n","!sudo apt-get install -y xvfb ffmpeg\n","!pip install imageio\n","!pip install PILLOW"],"execution_count":9,"outputs":[{"output_type":"stream","text":["shell-init: error retrieving current directory: getcwd: cannot access parent directories: No such file or directory\n","shell-init: error retrieving current directory: getcwd: cannot access parent directories: No such file or directory\n","The folder you are executing pip from can no longer be found.\n","shell-init: error retrieving current directory: getcwd: cannot access parent directories: No such file or directory\n","shell-init: error retrieving current directory: getcwd: cannot access parent directories: No such file or directory\n","The folder you are executing pip from can no longer be found.\n","shell-init: error retrieving current directory: getcwd: cannot access parent directories: No such file or directory\n","Reading package lists... Done\n","Building dependency tree       \n","Reading state information... Done\n","python-opengl is already the newest version (3.1.0+dfsg-1).\n","0 upgraded, 0 newly installed, 0 to remove and 25 not upgraded.\n","shell-init: error retrieving current directory: getcwd: cannot access parent directories: No such file or directory\n","Reading package lists... Done\n","Building dependency tree       \n","Reading state information... Done\n","xvfb is already the newest version (2:1.19.6-1ubuntu4.7).\n","0 upgraded, 0 newly installed, 0 to remove and 25 not upgraded.\n","shell-init: error retrieving current directory: getcwd: cannot access parent directories: No such file or directory\n","shell-init: error retrieving current directory: getcwd: cannot access parent directories: No such file or directory\n","The folder you are executing pip from can no longer be found.\n","shell-init: error retrieving current directory: getcwd: cannot access parent directories: No such file or directory\n","shell-init: error retrieving current directory: getcwd: cannot access parent directories: No such file or directory\n","The folder you are executing pip from can no longer be found.\n","shell-init: error retrieving current directory: getcwd: cannot access parent directories: No such file or directory\n","Reading package lists... Done\n","Building dependency tree       \n","Reading state information... Done\n","ffmpeg is already the newest version (7:3.4.8-0ubuntu0.2).\n","xvfb is already the newest version (2:1.19.6-1ubuntu4.7).\n","0 upgraded, 0 newly installed, 0 to remove and 25 not upgraded.\n","shell-init: error retrieving current directory: getcwd: cannot access parent directories: No such file or directory\n","shell-init: error retrieving current directory: getcwd: cannot access parent directories: No such file or directory\n","The folder you are executing pip from can no longer be found.\n","shell-init: error retrieving current directory: getcwd: cannot access parent directories: No such file or directory\n","shell-init: error retrieving current directory: getcwd: cannot access parent directories: No such file or directory\n","The folder you are executing pip from can no longer be found.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"5yTRSgI-ryd-","executionInfo":{"status":"ok","timestamp":1603384932106,"user_tz":420,"elapsed":341,"user":{"displayName":"Yiheng Li","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ggo9FSuWqtcs815uMwgcd6czNNqCSqb_lRojxQg=s64","userId":"15227785711110309391"}},"outputId":"b1b959b6-119a-413d-c8ad-6c62dd008b7e","colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["#@title Download Mujoco from an online repository\n","\n","MJC_PATH = '{}/mujoco'.format(SYM_PATH)\n","if not os.path.exists(MJC_PATH):\n","  %mkdir $MJC_PATH\n","%cd $MJC_PATH\n","if not os.path.exists(os.path.join(MJC_PATH, 'mujoco200')):\n","  !wget -q https://www.roboti.us/download/mujoco200_linux.zip\n","  !unzip -q mujoco200_linux.zip\n","  %mv mujoco200_linux mujoco200\n","  %rm mujoco200_linux.zip"],"execution_count":2,"outputs":[{"output_type":"stream","text":["/content/gdrive/My Drive/cs330_fall2020/mujoco\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"fQJfUoanr3pm","cellView":"both","executionInfo":{"status":"ok","timestamp":1603384933988,"user_tz":420,"elapsed":414,"user":{"displayName":"Yiheng Li","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ggo9FSuWqtcs815uMwgcd6czNNqCSqb_lRojxQg=s64","userId":"15227785711110309391"}}},"source":["#@title Important: ACTION Required BEFORE running this cell\n","#@markdown Place the mujoco key we have given you into a text file called mjkey.txt \n","#@markdown and ensure that the mujoco key is in the Google Drive path `cs330_fall2020/mujoco`.\n","\n","import os\n","\n","os.environ['LD_LIBRARY_PATH'] += ':{}/mujoco200/bin'.format(MJC_PATH)\n","os.environ['MUJOCO_PY_MUJOCO_PATH'] = '{}/mujoco200'.format(MJC_PATH)\n","os.environ['MUJOCO_PY_MJKEY_PATH'] = '{}/mjkey.txt'.format(MJC_PATH)\n","\n","## installation on colab does not find *.so files\n","## in LD_LIBRARY_PATH, copy over manually instead\n","!cp $MJC_PATH/mujoco200/bin/*.so /usr/lib/x86_64-linux-gnu/"],"execution_count":3,"outputs":[]},{"cell_type":"code","metadata":{"id":"IDYMDKI8hrHF","executionInfo":{"status":"ok","timestamp":1603384741827,"user_tz":420,"elapsed":62141,"user":{"displayName":"Yiheng Li","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ggo9FSuWqtcs815uMwgcd6czNNqCSqb_lRojxQg=s64","userId":"15227785711110309391"}},"outputId":"1dec0b8b-664a-42ce-ca7d-64f889263b18","colab":{"base_uri":"https://localhost:8080/","height":510}},"source":["#@title Important system updates for mujoco-py\n","!apt update \n","!apt install -y --no-install-recommends \\\n","        build-essential \\\n","        curl \\\n","        git \\\n","        gnupg2 \\\n","        make \\\n","        cmake \\\n","        ffmpeg \\\n","        swig \\\n","        libz-dev \\\n","        unzip \\\n","        zlib1g-dev \\\n","        libglfw3 \\\n","        libglfw3-dev \\\n","        libxrandr2 \\\n","        libxinerama-dev \\\n","        libxi6 \\\n","        libxcursor-dev \\\n","        libgl1-mesa-dev \\\n","        libgl1-mesa-glx \\\n","        libglew-dev \\\n","        libosmesa6-dev \\\n","        lsb-release \\\n","        ack-grep \\\n","        patchelf \\\n","        wget \\\n","        xpra \\\n","        xserver-xorg-dev \\\n","        xvfb \\\n","        python-opengl \\\n","        ffmpeg > /dev/null 2>&1"],"execution_count":5,"outputs":[{"output_type":"stream","text":["\u001b[33m\r0% [Working]\u001b[0m\r            \rIgn:1 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  InRelease\n","\u001b[33m\r0% [Connecting to archive.ubuntu.com (91.189.88.152)] [Connecting to security.u\u001b[0m\r                                                                               \rGet:2 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran40/ InRelease [3,626 B]\n","\u001b[33m\r0% [Connecting to archive.ubuntu.com (91.189.88.152)] [Connecting to security.u\u001b[0m\u001b[33m\r0% [Connecting to archive.ubuntu.com (91.189.88.152)] [Connecting to security.u\u001b[0m\u001b[33m\r0% [2 InRelease gpgv 3,626 B] [Connecting to archive.ubuntu.com (91.189.88.152)\u001b[0m\r                                                                               \rIgn:3 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  InRelease\n","\u001b[33m\r0% [2 InRelease gpgv 3,626 B] [Connecting to archive.ubuntu.com (91.189.88.152)\u001b[0m\r                                                                               \rGet:4 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Release [697 B]\n","\u001b[33m\r0% [2 InRelease gpgv 3,626 B] [Connecting to archive.ubuntu.com (91.189.88.152)\u001b[0m\u001b[33m\r0% [2 InRelease gpgv 3,626 B] [Connecting to archive.ubuntu.com (91.189.88.152)\u001b[0m\r                                                                               \rHit:5 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  Release\n","\u001b[33m\r0% [2 InRelease gpgv 3,626 B] [Connecting to archive.ubuntu.com (91.189.88.152)\u001b[0m\r                                                                               \rGet:6 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Release.gpg [836 B]\n","Get:7 http://security.ubuntu.com/ubuntu bionic-security InRelease [88.7 kB]\n","Get:8 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic InRelease [15.9 kB]\n","Hit:9 http://archive.ubuntu.com/ubuntu bionic InRelease\n","Get:11 http://archive.ubuntu.com/ubuntu bionic-updates InRelease [88.7 kB]\n","Ign:12 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Packages\n","Get:12 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Packages [335 kB]\n","Hit:13 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic InRelease\n","Get:14 http://archive.ubuntu.com/ubuntu bionic-backports InRelease [74.6 kB]\n","Get:15 http://security.ubuntu.com/ubuntu bionic-security/restricted amd64 Packages [211 kB]\n","Get:16 http://security.ubuntu.com/ubuntu bionic-security/multiverse amd64 Packages [15.4 kB]\n","Get:17 http://security.ubuntu.com/ubuntu bionic-security/main amd64 Packages [1,745 kB]\n","Get:18 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic/main Sources [1,681 kB]\n","Get:19 http://security.ubuntu.com/ubuntu bionic-security/universe amd64 Packages [1,352 kB]\n","Get:20 http://archive.ubuntu.com/ubuntu bionic-updates/multiverse amd64 Packages [45.9 kB]\n","Get:21 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 Packages [2,115 kB]\n","Get:22 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 Packages [2,162 kB]\n","Get:23 http://archive.ubuntu.com/ubuntu bionic-updates/restricted amd64 Packages [238 kB]\n","Get:24 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic/main amd64 Packages [860 kB]\n","Fetched 11.0 MB in 4s (3,079 kB/s)\n","Reading package lists... Done\n","Building dependency tree       \n","Reading state information... Done\n","25 packages can be upgraded. Run 'apt list --upgradable' to see them.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"tcG4cdIysCu_","executionInfo":{"status":"ok","timestamp":1603384910343,"user_tz":420,"elapsed":16357,"user":{"displayName":"Yiheng Li","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ggo9FSuWqtcs815uMwgcd6czNNqCSqb_lRojxQg=s64","userId":"15227785711110309391"}},"outputId":"8bff1356-416e-4b73-e717-04d2ee3f1e27","colab":{"base_uri":"https://localhost:8080/","height":445}},"source":["#@title Clone and install mujoco-py\n","#@markdown Remember that you need to put the key in the appropriate location as described above\n","%cd $MJC_PATH\n","if not os.path.exists('mujoco-py'):\n","  !git clone https://github.com/openai/mujoco-py.git\n","%cd mujoco-py\n","%pip install -e ."],"execution_count":6,"outputs":[{"output_type":"stream","text":["/content/gdrive/My Drive/cs330_fall2020/mujoco\n","/content/gdrive/My Drive/cs330_fall2020/mujoco/mujoco-py\n","Obtaining file:///content/gdrive/My%20Drive/cs330_fall2020/mujoco/mujoco-py\n","  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n","\u001b[33m  WARNING: Missing build requirements in pyproject.toml for file:///content/gdrive/My%20Drive/cs330_fall2020/mujoco/mujoco-py.\u001b[0m\n","\u001b[33m  WARNING: The project does not specify a build backend, and pip cannot fall back to setuptools without 'wheel'.\u001b[0m\n","  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n","  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n","    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: fasteners~=0.15 in /usr/local/lib/python3.6/dist-packages (from mujoco-py==2.0.2.13) (0.15)\n","Requirement already satisfied: Cython>=0.27.2 in /usr/local/lib/python3.6/dist-packages (from mujoco-py==2.0.2.13) (0.29.21)\n","Requirement already satisfied: numpy>=1.11 in /usr/local/lib/python3.6/dist-packages (from mujoco-py==2.0.2.13) (1.18.5)\n","Requirement already satisfied: cffi>=1.10 in /usr/local/lib/python3.6/dist-packages (from mujoco-py==2.0.2.13) (1.14.3)\n","Requirement already satisfied: glfw>=1.4.0 in /usr/local/lib/python3.6/dist-packages (from mujoco-py==2.0.2.13) (2.0.0)\n","Requirement already satisfied: imageio>=2.1.2 in /usr/local/lib/python3.6/dist-packages (from mujoco-py==2.0.2.13) (2.4.1)\n","Requirement already satisfied: monotonic>=0.1 in /usr/local/lib/python3.6/dist-packages (from fasteners~=0.15->mujoco-py==2.0.2.13) (1.5)\n","Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from fasteners~=0.15->mujoco-py==2.0.2.13) (1.15.0)\n","Requirement already satisfied: pycparser in /usr/local/lib/python3.6/dist-packages (from cffi>=1.10->mujoco-py==2.0.2.13) (2.20)\n","Requirement already satisfied: pillow in /usr/local/lib/python3.6/dist-packages (from imageio>=2.1.2->mujoco-py==2.0.2.13) (7.0.0)\n","Installing collected packages: mujoco-py\n","  Found existing installation: mujoco-py 2.0.2.13\n","    Can't uninstall 'mujoco-py'. No files were found to uninstall.\n","  Running setup.py develop for mujoco-py\n","Successfully installed mujoco-py\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Z92eTDkLSZTu"},"source":["## cythonize at the first import\n","import mujoco_py"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"p0U9ouFusPp1","executionInfo":{"status":"ok","timestamp":1603252563046,"user_tz":420,"elapsed":270,"user":{"displayName":"Yiheng Li","photoUrl":"","userId":"15227785711110309391"}},"outputId":"b25f5de3-4315-4297-f77d-73f1c27611a0","colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["os.listdir(\"/content/cs330_fall2020/mujoco/\")"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['mujoco200', 'mujoco-py', 'mjkey.txt', 'MUJOCO_LOG.TXT']"]},"metadata":{"tags":[]},"execution_count":6}]},{"cell_type":"code","metadata":{"id":"AGGFqjdcsX3g","executionInfo":{"status":"ok","timestamp":1603243511687,"user_tz":420,"elapsed":13534,"user":{"displayName":"Yiheng Li","photoUrl":"","userId":"15227785711110309391"}},"outputId":"72da6ee7-5ad2-4142-abc7-c7d56150353b","colab":{"base_uri":"https://localhost:8080/","height":272}},"source":["#@title Clone and install multiworld\n","%cd $SYM_PATH\n","!git clone https://github.com/vitchyr/multiworld.git\n","\n","%cd multiworld\n","%pip install -e .\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[Errno 2] No such file or directory: '$SYM_PATH'\n","/content\n","Cloning into 'multiworld'...\n","remote: Enumerating objects: 155, done.\u001b[K\n","remote: Counting objects: 100% (155/155), done.\u001b[K\n","remote: Compressing objects: 100% (104/104), done.\u001b[K\n","remote: Total 12652 (delta 86), reused 95 (delta 51), pack-reused 12497\u001b[K\n","Receiving objects: 100% (12652/12652), 87.83 MiB | 24.08 MiB/s, done.\n","Resolving deltas: 100% (8575/8575), done.\n","Checking out files: 100% (510/510), done.\n","/content/multiworld\n","Obtaining file:///content/multiworld\n","Installing collected packages: multiworld\n","  Running setup.py develop for multiworld\n","Successfully installed multiworld\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"0q6Swy46pzyg","executionInfo":{"status":"ok","timestamp":1603252573125,"user_tz":420,"elapsed":504,"user":{"displayName":"Yiheng Li","photoUrl":"","userId":"15227785711110309391"}},"outputId":"8b07ffd3-5e72-4c2e-fe51-f23002c5d7ae","colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["#@title Sets up virtual display\n","from pyvirtualdisplay import Display\n","display = Display(visible=0, size=(1400, 900))\n","display.start()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<pyvirtualdisplay.display.Display at 0x7fa13e55d0f0>"]},"metadata":{"tags":[]},"execution_count":7}]},{"cell_type":"code","metadata":{"id":"2Zc9_-pRitwk"},"source":["#@title Check imports and add helper functions for display\n","\n","import os\n","import gym\n","from gym import logger as gymlogger\n","from gym.wrappers import Monitor\n","gymlogger.set_level(40) # error only\n","import tensorflow as tf\n","import numpy as np\n","import random\n","import matplotlib\n","import matplotlib.pyplot as plt\n","import math\n","import glob\n","import io\n","import base64\n","from IPython.display import HTML\n","\n","from IPython import display as ipythondisplay\n","if type(os.environ.get(\"DISPLAY\")) is not str or len(os.environ.get(\"DISPLAY\"))==0:\n","    !bash ../xvfb start\n","    %env DISPLAY=:1\n","\n","def show_video():\n","  mp4list = glob.glob('video/*.mp4')\n","  if len(mp4list) > 0:\n","    mp4 = mp4list[0]\n","    video = io.open(mp4, 'r+b').read()\n","    encoded = base64.b64encode(video)\n","    ipythondisplay.display(HTML(data='''<video alt=\"test\" autoplay \n","                loop controls style=\"height: 400px;\">\n","                <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\" />\n","             </video>'''.format(encoded.decode('ascii'))))\n","  else: \n","    print(\"Could not find video\")\n","    \n","\n","def wrap_env(env):\n","  env = Monitor(env, './video', force=True)\n","  return env"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"c3uYa0KHs0LB"},"source":["#@title After running, you should see a video play\n","matplotlib.use('Agg')\n","\n","env = wrap_env(gym.make(\"Ant-v2\"))\n","\n","observation = env.reset()\n","for i in range(10):\n","    env.render(mode='rgb_array')\n","    obs, rew, term, _ = env.step(env.action_space.sample() ) \n","    if term:\n","      break;\n","            \n","env.close()\n","print('Loading video...')\n","show_video()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"v6uncjbtGb_s"},"source":["#@title Random seed is set to be fixed\n","import tensorflow\n","tensorflow.random.set_seed(330)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-mD5hrg3cPn7"},"source":["# BitFlip Goal Conditioned RL\n","\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"PpGZxJIg9ghR"},"source":["## BitFlipEnv\n","\n","Familiarize yourself with what the bit flip environment does and what each method does.\n","\n","You do *NOT* need to modify the following cell."]},{"cell_type":"code","metadata":{"id":"kKw1cs-ZiAjt"},"source":["class BitFlipEnv():\n","    '''bit flipping environment for reinforcement learning.\n","    The environment is a 1D vector of binary values (state vector).\n","    At each step, the actor can flip a single bit (0 to 1 or 1 to 0).\n","    The goal is to flip bits until the state vector matches the\n","    goal vector (also a 1D vector of binary values). At each step,\n","    the actor receives a goal of 0 if the state and goal vector\n","    do not match and a reward of 1 if the state and goal vector\n","    match.\n","\n","    Internally the state and goal vector are a numpy array, which\n","    allows the vectors to be printed by the show_goal and show_state\n","    methods. When '''\n","\n","    def __init__(self, num_bits, verbose = False):\n","        '''Initialize new instance of BitFlip class.\n","        inputs: num_bits - number of bits in the environment; must\n","                be an integer\n","                verbose - prints state and goal vector after each\n","                          step if True'''\n","\n","        # check that num_bits is a positive integer\n","        if (num_bits < 0) or (type(num_bits) != type(0)):\n","            print(\"Invalid number of bits -  must be positive integer\")\n","            return\n","\n","        # number of bits in the environment\n","        self.num_bits = num_bits\n","        # randomly set the state vector\n","        self.state_vector = np.random.randint(0, 2, num_bits)\n","        # randomly set the goal vector\n","        self.goal_vector = np.random.randint(0, 2, num_bits)\n","        # whether to print debugging info\n","        self.verbose = verbose\n","        # TODO set dimensions of observation space\n","        self.observation_space = self.state_vector\n","        # TODO create action space; may use gym type\n","        self.action_space = num_bits\n","        # space of the goal vector\n","        self.goal_space = self.goal_vector\n","        # number of steps taken\n","        self.steps = 0\n","\n","        return\n","\n","    def show_goal(self):\n","        '''Returns the goal as a numpy array. Used for debugging.'''\n","        return self.goal_vector\n","\n","    def show_state(self):\n","        '''Returns the state as a numpy array. Used for debugging.'''\n","        return self.state_vector\n","\n","    def reset(self):\n","        '''resets the environment. Returns a reset state_vector\n","        and goal_vector as tf tensors'''\n","\n","        # randomly reset both the state and the goal vectors\n","        self.state_vector = np.random.randint(0, 2, self.num_bits)\n","        self.goal_vector = np.random.randint(0, 2, self.num_bits)\n","        self.steps = 0\n","\n","        # return as np array\n","        return self.state_vector, self.goal_vector\n","\n","\n","    def step(self, action):\n","        '''take a step and flip one of the bits.\n","\n","        inputs: action - integer index of the bit to flip\n","        outputs: state - new state_vector (tensor)\n","                 reward - 0 if state != goal and 1 if state == goal\n","                 done - boolean value indicating if the goal has been reached'''\n","        self.steps += 1\n","\n","\n","        if action < 0 or action >= self.num_bits:\n","            # check argument is in range\n","            print(\"Invalid action! Must be integer ranging from \\\n","                0 to num_bits-1\")\n","            return\n","\n","        # flip the bit with index action\n","        if self.state_vector[action] == 1:\n","            self.state_vector[action] = 0\n","        else:\n","            self.state_vector[action] = 1\n","\n","        # initial values of reward and done - may change\n","        # depending on state and goal vectors\n","        reward = 0\n","        done = True\n","\n","        # check if state and goal vectors are identical\n","        if False in (self.state_vector == self.goal_vector):\n","            reward = -1\n","            done = False\n","\n","        # print additional info if verbose mode is on\n","        if self.verbose:\n","            print(\"Bit flipped:   \", action)\n","            print(\"Goal vector:   \", self.goal_vector)\n","            print(\"Updated state: \", self.state_vector)\n","            print(\"Reward:        \", reward)\n","\n","        if done:\n","            #print(\"Solved in: \", self.steps)\n","            pass\n","\n","        # return state as numpy arrays\n","        # return goal_vector in info field\n","        return np.copy(self.state_vector), reward, done, self.steps\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"GZOi6L7Pdgd0"},"source":["## Buffer\n","Familiarize yourself with what the buffer does \n","\n","You do *NOT* need to modify the following cell."]},{"cell_type":"code","metadata":{"id":"J14LUXqwkC9e"},"source":["import numpy as np\n","import random\n","from collections import deque \n","\n","class Buffer(object) :\n","\n","\tdef __init__(self,size,sample_size):\n","\n","\t\tself.size = size\n","\t\tself.sample_size = sample_size\n","\t\tself.buffer = deque()\n","\n","\tdef add(self,state,action,reward,next_state) :\n","\t\tself.buffer.append((state,action,reward,next_state))\n","\n","\t\tif len(self.buffer) > self.size:\n","\t\t\tself.buffer.popleft()\n","\n","\tdef sample(self) :\n","\t\tif len(self.buffer) < self.sample_size:\n","\t\t\tsamples = self.buffer\n","\t\telse:\t\n","\t\t\tsamples = random.sample(self.buffer,self.sample_size)\n","\t\t\n","\t\tstate = np.reshape(np.array([arr[0] for arr in samples]),[len(samples),-1])\n","\t\taction = np.array([arr[1] for arr in samples])\n","\t\treward = np.array([arr[2] for arr in samples])\n","\t\tnext_state = np.reshape(np.array([arr[3] for arr in samples]),[len(samples),-1])\n","\n","\t\treturn state, action, reward, next_state\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"JPtYOkhghoz-"},"source":["## BitFlip Goal Condition RL and Training\n","\n","Implement the changes you need for Problems 1-3 here in the cells below."]},{"cell_type":"code","metadata":{"id":"jgoB2zVqi2G8"},"source":["import numpy as np\n","import tensorflow as tf\n","from matplotlib import pyplot as plt\n","\n","\n","class Model(tf.keras.Model):\n","\n","  def __init__(self, num_bits):\n","    super(Model, self).__init__()\n","\n","    hidden_dim = 256\n","    self.dense1 = tf.keras.layers.Dense(hidden_dim, activation=tf.nn.relu)\n","    self.out = tf.keras.layers.Dense(num_bits,activation = None)\n","\n","  def call(self, inputs):\n","\n","    x = self.dense1(inputs)\n","    return self.out(x)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"rXD2QYkHh56I"},"source":["# ************   Helper functions    ************ #\n","\n","def updateTarget(model, target_model) :\n","    target_model.set_weights(model.get_weights()) \n","\n","def solve_environment(num_bits, model, bit_env, state, goal_state, total_reward):\n","    '''attempt to solve the bit flipping environment using the current policy\n","\n","    inputs: num_bits - number of bits to be looped over \n","        model - DQN to run prediction on\n","        bit_env - environment for bitflip\n","        state - current state\n","        goal_state - desired state\n","        total_reward - cumulative reward so far\n","    '''\n","    \n","    # list for recording what happened in the episode\n","    episode_experience = []\n","    succeeded = False\n","\n","    for t in range(num_bits):\n","      \n","      # attempt to solve the state - number of steps given to solve the\n","      # state is equal to the size of the vector\n","      \n","      # ======================== TODO modify code ========================\n","      1/0\n","      # forward pass to find action\n","      action = model(state)\n","      # add to the episode experience (what happened)\n","      episode_experience.append(action)\n","      # calculate total reward\n","      total_reward = total_reward + \n","      # update state\n","      state = None\n","      # mark that we've finished the episode and succeeded with training\n","      if succeeded:\n","        \n","\n","      # ========================      END TODO       ========================\n","\n","\n","    return succeeded, episode_experience, total_reward\n","\n","def solve_environment_no_goal(num_bits, model, bit_env, state, goal_state, total_reward):\n","    '''attempt to solve the bit_flip env using no goal'''\n","    \n","    # list for recording what happened in the episode\n","    episode_experience = []\n","    succeeded = False\n","\n","    for t in range(num_bits):\n","        # attempt to solve the state - number of steps given to solve the\n","        # state is equal to the passed argument steps_per_episode.\n","\n","        inputs = state\n","        inputs = np.expand_dims(inputs, axis=0)\n","        # forward pass to find action\n","        out = model(inputs)\n","        action = np.argmax(out,axis = 1)\n","        # take the action\n","        next_state,reward,done, _ = bit_env.step(action)\n","        # add to the episode experience (what happened)\n","        episode_experience.append((state,action,reward,next_state,goal_state))\n","        # calculate total reward\n","        total_reward+=reward\n","        # update state\n","        state = next_state\n","        # mark that we've finished the episode and succeeded with training\n","        if done:\n","            if succeeded:\n","                continue\n","            else:\n","                succeeded = True\n","\n","\n","\n","    return succeeded, episode_experience, total_reward\n","\n","\n","def update_replay_buffer(num_bits, num_relabeled, replay_buffer, episode_experience, HER):\n","    '''adds past experience to the replay buffer. Training is done with episodes from the replay\n","    buffer. When HER is used, relabeled experiences are also added to the replay buffer\n","\n","    inputs: num_bits - number of bits to be looped over \n","            replay_buffer - the buffer to store past experience in\n","            episode_experience - list of transitions from the last episode\n","            HER -  type of hindsight experience replay to use\n","    modifies: replay_buffer\n","    outputs: None'''\n","\n","    for t in range(num_bits) :\n","        # copy actual experience from episode_experience to replay_buffer\n","\n","        # ======================== TODO modify code ========================\n","        s,a,r,s_,g = episode_experience[t]\n","        # state\n","        inputs = s,\n","        # next state\n","        inputs_ = s_\n","        # add to the replay buffer\n","        replay_buffer.add(inputs,a,r,inputs_)\n","\n","        # when HER is used, each call to update_replay_buffer should add num_relabeled\n","        # relabeled points to the replay buffer\n","\n","        if HER == 'None':\n","            # HER not being used, so do nothing\n","            pass\n","\n","        elif HER == 'final':\n","            # final - relabel based on final state in episode\n","            pass\n","\n","        elif HER == 'future':\n","            # future - relabel based on future state. At each timestep t, relabel the\n","            # goal with a randomly select timestep between t and the end of the\n","            # episode\n","            pass\n","\n","        elif HER == 'random':\n","             # random - relabel based on a random state in the episode\n","            pass\n","\n","        # ========================      END TODO       ========================\n","\n","\n","        else:\n","            print(\"Invalid value for Her flag - HER not used\")\n","    return\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"WL4Cedzih-eg"},"source":["\n","# ************   Main training loop    ************ #\n","\n","\n","def flip_bits(num_bits, num_epochs, buffer_size = 1e6, batch_size = 128, \n","              num_episodes = 16, num_relabeled = 4, gamma = 0.98, log_interval=5, opt_steps=40, HER = \"None\"):\n","    '''Main loop for running in the bit flipping environment. The DQN is\n","    trained over num_epochs. In each epoch, the agent runs in the environment\n","    num_episodes number of times. The Q-target and Q-policy networks are\n","    updated at the end of each epoch. Within one episode, Q-policy attempts\n","    to solve the environment and is limited to the same number as steps as the\n","    size of the environment\n","\n","    inputs: HER - string specifying whether to use HER'''\n","\n","    print(\"Running bit flip environment with %d bits and HER policy: %s\" %(num_bits, HER))\n","\n","    # create bit flipping environment and replay buffer\n","    bit_env = BitFlipEnv(num_bits)\n","    replay_buffer = Buffer(buffer_size,batch_size)\n","\n","    # set up Q-policy (model) and Q-target (target_model)\n","    model = Model(num_bits)\n","    optimizer = tf.keras.optimizers.Adam(learning_rate = 1e-3)\n","    target_model = Model(num_bits)\n","\n","    # ======================== TODO modify code ========================\n","    # modify to be goal conditioned\n","    state, goal_state = bit_env.reset()      \n","    inputs = state\n","    \n","    inputs = np.expand_dims(inputs, axis=0)  \n","    model(inputs)\n","    target_model(inputs)\n","\n","    # start by making Q-target and Q-policy the same\n","    updateTarget(model, target_model)\n","    # ========================      END TODO       ========================\n","\n","\n","    total_loss = []                  # training loss for each epoch\n","    success_rate = []                # success rate for each epoch\n","    \n","    for i in range(num_epochs):\n","        # Run for a fixed number of epochs\n","\n","        total_reward = 0.0           # total reward for the epoch\n","        successes = []               # record success rate for each episode of the epoch\n","        losses = []                  # loss at the end of each epoch\n","\n","        for k in range(num_episodes):\n","            # Run in the environment for num_episodes  \n","            state, goal_state = bit_env.reset()             # reset the environment     \n","            # attempt to solve the environment\n","            # ======================== TODO modify code ========================\n","            # modify to be goal conditioned\n","            succeeded, episode_experience, total_reward = solve_environment_no_goal(num_bits, model, bit_env, state, goal_state, total_reward)\n","            # ========================     END TODO     ========================\n","            successes.append(succeeded)                     # track whether we succeeded in environment \n","            update_replay_buffer(num_bits, num_relabeled, replay_buffer, episode_experience, HER)   # add to the replay buffer; use specified  HER policy\n","        for k in range(opt_steps):\n","            # optimize the Q-policy network\n","\n","            # sample from the replay buffer\n","            state,action,reward,next_state = replay_buffer.sample()\n","            # forward pass through target network   \n","            # target_net_Q = sess.run(target_model.out,feed_dict = {target_model.inp : next_state})\n","            with tf.GradientTape() as tape:\n","              target_net_Q = target_model(next_state)\n","              # calculate target reward\n","              target_reward = np.clip(np.reshape(reward,[-1]) + gamma * np.reshape(np.max(target_net_Q,axis = -1),[-1]),-1. / (1 - gamma), 0)\n","              # calculate predictions and loss\n","              model_predict = model(state)\n","              model_action_taken = np.reshape(action,[-1])\n","              action_one_hot = tf.one_hot(model_action_taken, num_bits)\n","              Q_val = tf.reduce_sum(model_predict * action_one_hot, axis=1)\n","              loss = tf.reduce_mean(tf.square(Q_val - target_reward))\n","              losses.append(loss)\n","            \n","            gradients = tape.gradient(loss, model.trainable_variables)\n","            optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n","            \n","        updateTarget(model, target_model)               # update target model by copying Q-policy to Q-target      \n","        success_rate.append(np.mean(successes))       # append mean success rate for this epoch\n","\n","        if i % log_interval == 0:\n","            print('Epoch: %d  Cumulative reward: %f  Success rate: %.4f Mean loss: %.4f' % (i, total_reward, np.mean(successes), np.mean(losses)))\n","                \n","    return success_rate"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"uaqw28hxi6Zi"},"source":["# Sample commands have been provided to you below\n","# run with type of HER specified\n","success_rate  = flip_bits(num_bits=7, num_epochs=150, HER='None') \n","# pass success rate for each run as first argument and labels as second list"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"lua6jJR-cnv2"},"source":["# Sawyer Environment Goal-Conditioned RL"]},{"cell_type":"code","metadata":{"id":"7ojAJB_4colS"},"source":["#@title Buffer\n","#@markdown Same as the Buffer class before but placed here in the event you want to run the sections separately\n","import numpy as np\n","import random\n","from collections import deque \n","\n","class Buffer(object) :\n","\n","\tdef __init__(self,size,sample_size):\n","\n","\t\tself.size = size\n","\t\tself.sample_size = sample_size\n","\t\tself.buffer = deque()\n","\n","\tdef add(self,state,action,reward,next_state) :\n","\t\tself.buffer.append((state,action,reward,next_state))\n","\n","\t\tif len(self.buffer) > self.size:\n","\t\t\tself.buffer.popleft()\n","\n","\tdef sample(self) :\n","\t\tif len(self.buffer) < self.sample_size:\n","\t\t\tsamples = self.buffer\n","\t\telse:\t\n","\t\t\tsamples = random.sample(self.buffer,self.sample_size)\n","\t\t\n","\t\tstate = np.reshape(np.array([arr[0] for arr in samples]),[len(samples),-1])\n","\t\taction = np.array([arr[1] for arr in samples])\n","\t\treward = np.array([arr[2] for arr in samples])\n","\t\tnext_state = np.reshape(np.array([arr[3] for arr in samples]),[len(samples),-1])\n","\n","\t\treturn state, action, reward, next_state\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"V4b_b31-lQFp"},"source":["import numpy as np\n","import tensorflow as tf\n","from matplotlib import pyplot as plt\n","import multiworld\n","import glfw\n","\n","multiworld.register_all_envs()   \n","\n","class Model(tf.keras.Model):\n","\n","  def __init__(self, num_act):\n","    super(Model, self).__init__()\n","\n","    hidden_dim = 256\n","    self.dense1 = tf.keras.layers.Dense(hidden_dim, activation=tf.nn.relu)\n","    self.out = tf.keras.layers.Dense(num_act,activation = None)\n","\n","  def call(self, inputs):\n","    x = self.dense1(inputs)\n","    return self.out(x)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"AVtTlZFSlSi2"},"source":["# ************   Helper functions    ************ #\n","# Globals\n","\n","NUM_DIM = 2\n","NUM_ACT = 4\n","done_threshold = -0.01\n","Sawyer_Env = env = wrap_env(gym.make('SawyerReachXYEnv-v1'))\n","\n","def take_action(action, render):\n","    '''passes the discrete action selected by the Q-network to the Sawyer Arm.\n","    The function returns the next state, the reward, and whether the environment\n","    was solved. The environment done returned is not the same as the environment\n","    done returned by the Sawyer environment. Due to discretization, it may not be\n","    possible to exactly reach the goal. The done flag returns true if the end\n","    state is within done_threshold of the final goal\n","\n","    inputs:  action - integer (0 to NUM_ACT-1) selected by the Q-network\n","    outputs: next_state - new state (x, y) location of arm\n","             reward - reward returned by Sawyer environment\n","             done - boolean whether environment is solved'''\n","\n","    # maps actions selected by Q-network to Sawyer arm actions\n","    # array MUST be length NUM_ACT\n","    action_dic = {0:[-1, 0], 1:[1, 0], 2:[0, -1], 3:[0, 1]}\n","    # look up which action in Sawyer arm space corresponds to the selected integer action\n","    action_sawyer = np.array(action_dic[action], dtype=np.float32)\n","    # take the action\n","    ob, reward, done, info = Sawyer_Env.step(action_sawyer)\n","    # if rendering is turned on, render the environment\n","    if render:\n","        Sawyer_Env.render(mode='rgb_array')\n","    # check if we're \"close enough\" to declare done\n","    if reward > done_threshold:\n","        done = True\n","\n","    # pull the observed state off\n","    next_state = ob['observation'][0:2]\n","\n","    return next_state, reward, done, info\n","\n","def solve_environment(model, state, goal_state, total_reward, steps_per_episode, render):\n","    '''attempt to solve the Sawyer Arm environment using the current policy'''\n","    \n","    # list for recording what happened in the episode\n","    episode_experience = []\n","    succeeded = False\n","\n","    for t in range(steps_per_episode):\n","      # attempt to solve the state - number of steps given to solve the\n","      # state is equal to the passed argument steps_per_episode.\n","\n","      # ======================== TODO modify code ========================\n","      continue\n","      # forward pass to find action\n","\n","      # take the action - use helper function to convert discrete actions to\n","      # actions in the Sawyer environment\n","\n","      # add to the episode experience (what happened)\n","\n","      # calculate total reward\n","\n","      # update state\n","\n","      # mark that we've finished the episode and succeeded with training\n","\n","      # ========================      END TODO       ========================\n","\n","\n","    return succeeded, episode_experience, total_reward\n","\n","\n","def solve_environment_no_goal(model, state, goal_state, total_reward, steps_per_episode, render):\n","    '''attempt to solve the Sawyer Arm environment using the current policy with no goal condition'''\n","    \n","    # list for recording what happened in the episode\n","    episode_experience = []\n","    succeeded = False\n","\n","    for t in range(steps_per_episode):\n","        inputs = state\n","        inputs = np.expand_dims(inputs, axis=0)\n","        inputs = np.array(inputs, dtype=np.float32) \n","        # forward pass to find action\n","        out = model(inputs)\n","        action = np.argmax(out,axis = 1)[0]\n","        next_state,reward,done, _ = take_action(action, render)\n","        # add to the episode experience (what happened)\n","        episode_experience.append((state,action,reward,next_state,goal_state))\n","        # calculate total reward\n","        total_reward += reward\n","        # update state\n","        state = next_state\n","        # mark that we've finished the episode and succeeded with training\n","        if done:\n","            if succeeded:\n","                continue\n","            else:\n","                succeeded = True\n","    else:\n","         env.stats_recorder.save_complete()\n","         env.stats_recorder.done = True\n","\n","    return succeeded, episode_experience, total_reward\n","\n","def update_replay_buffer(steps_per_episode, num_relabeled, replay_buffer, episode_experience, HER):\n","    '''adds past experience to the replay buffer. Training is done with episodes from the replay\n","    buffer. When HER is used, num_relabeled additional relabeled data points are also added\n","    to the replay buffer\n","\n","    inputs:    epsidode_experience - list of transitions from the last episode\n","    modifies:  replay_buffer\n","    outputs:   None'''\n","\n","    for t in range(steps_per_episode) :\n","        # copy actual experience from episode_experience to replay_buffer\n","\n","        # ======================== TODO modify code ========================\n","        s,a,r,s_,g = episode_experience[t]\n","        # state\n","        inputs = s\n","        # next state\n","        inputs_ = s_\n","        # add to the replay buffer\n","        replay_buffer.add(inputs,a,r,inputs_)\n","\n","\n","        # when HER is used, each call to update_replay_buffer should add num_relabeled\n","        # relabeled points to the replay buffer\n","        if HER == 'None':\n","            # HER not being used, so do nothing\n","            pass\n","\n","        elif HER == 'final':\n","            # final - relabel based on final state in episode\n","            pass\n","\n","        elif HER == 'future':\n","            # future - relabel based on future state. At each timestep t, relabel the\n","            # goal with a randomly select timestep between t and the end of the\n","            # episode\n","            pass\n","\n","        elif HER == 'random':\n","            # random - relabel based on a random state in the episode\n","            pass\n","\n","\n","        # ========================      END TODO       ========================\n","\n","        else:\n","            print(\"Invalid value for Her flag - HER not used\")\n","    return\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"xX0tbP_w9ViT"},"source":["# ************   Main Training Loop    ************ #\n","\n","def run_sawyer(num_epochs, buffer_size = 1e6, batch_size = 128, \n","               num_episodes = 16, num_relabeled = 4, gamma = 0.98, log_interval=5, opt_steps=40,\n","               steps_per_episode=50, render=False, HER = \"None\"):\n","    '''Main loop for running in the Sawyer environment. The DQN is\n","    trained over num_epochs. In each epoch, the agent runs in the environment\n","    num_episodes number of times. The Q-target and Q-policy networks are\n","    updated at the end of each epoch. Within one episode, Q-policy attempts\n","    to solve the environment and is limited to the same number as steps as the\n","    size of the environment\n","\n","    inputs: HER - string specifying whether to use HER'''\n","    # create Sawyer arm environment and replay buffer\n","    replay_buffer = Buffer(buffer_size,batch_size)\n","\n","    # set up Q-policy (model) and Q-target (target_model)\n","    model = Model(NUM_ACT)\n","    target_model = Model(NUM_ACT)\n","\n","    optimizer = tf.keras.optimizers.Adam(learning_rate = 1e-3)\n","\n","    # ======================== TODO modify code ========================\n","    # modify to be goal conditioned\n","    reset_state = Sawyer_Env.reset()  \n","    state = reset_state['observation'][:2]          # look up the state\n","    \n","    inputs = np.expand_dims(state, axis=0)  \n","    model(inputs)\n","    target_model(inputs)\n","\n","    # ========================      END TODO       ========================\n","\n","\n","    # start by making Q-target and Q-policy the same\n","    updateTarget(model, target_model)\n","\n","\n","    total_loss = []                  # training loss for each epoch\n","    success_rate = []                # success rate for each epoch\n","    \n","    for i in range(num_epochs):\n","        # Run for a fixed number of epochs\n","\n","        total_reward = 0.0           # total reward for the epoch\n","        successes = []               # record success rate for each episode of the epoch\n","        losses = []                  # loss at the end of each epoch\n","\n","        for k in range(num_episodes):\n","            reset_state = Sawyer_Env.reset()                # reset the environment\n","            state = reset_state['observation'][:2]          # look up the state\n","            goal_state = reset_state['desired_goal'][:2]    # look up the goal\n","\n","            # attempt to solve the environment\n","            # ======================== TODO modify code ========================\n","            # modify to be goal conditioned\n","            succeeded, episode_experience, total_reward = solve_environment_no_goal(model, state, goal_state, total_reward, steps_per_episode, render)\n","            # ========================      END TODO       ========================\n","\n","            successes.append(succeeded)                     # track whether we succeeded in environment \n","            update_replay_buffer(steps_per_episode, num_relabeled, replay_buffer, episode_experience, HER)   # add to the replay buffer; use specified  HER policy\n","            env.close() \n","            glfw.terminate()\n","        for k in range(opt_steps):\n","            # optimize the Q-policy network\n","\n","            # sample from the replay buffer\n","            state,action,reward,next_state = replay_buffer.sample()\n","            state = np.array(state, dtype=np.float32) \n","            next_state = np.array(next_state, dtype=np.float32) \n","            # forward pass through target network   \n","\n","            with tf.GradientTape() as tape:\n","              target_net_Q = target_model(next_state)\n","              # calculate target reward\n","              target_reward = np.clip(np.reshape(reward,[-1]) + gamma * np.reshape(np.max(target_net_Q,axis = -1),[-1]),-1. / (1 - gamma), 0)\n","              # calculate loss\n","              model_predict = model(state)\n","              model_action_taken = np.reshape(action,[-1])\n","              action_one_hot = tf.one_hot(model_action_taken, NUM_ACT)\n","              Q_val = tf.reduce_sum(model_predict * action_one_hot, axis=1)\n","              loss = tf.reduce_mean(tf.square(Q_val - target_reward))\n","              losses.append(loss)\n","            \n","            gradients = tape.gradient(loss, model.trainable_variables)\n","            optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n","            \n","        updateTarget(model, target_model)               # update target model by copying Q-policy to Q-target      \n","        success_rate.append(np.mean(successes))       # append mean success rate for this epoch\n","\n","        if i % log_interval == 0:\n","            print('Epoch: %d  Cumulative reward: %f  Success rate: %.4f Mean loss: %.4f' % (i, total_reward, np.mean(successes), np.mean(losses)))\n","    return success_rate"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"IMNnVQsNAgni"},"source":["success_rate = run_sawyer(num_epochs=150, HER = \"None\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Sbhq3UiESmon"},"source":["# If you chose to render:\n","show_video()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"2UKXENISPtf5"},"source":["# Plotting Code\n","\n","We've provided some sample plotting code for you. Feel free to customize it per the assignment specifications. The code will not be graded."]},{"cell_type":"code","metadata":{"id":"NOzYeO5EAo6c"},"source":["pip install plotly"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"OrP-V9ZJHW-3"},"source":["from IPython.display import HTML\n","from plotly import graph_objs as go\n","\n","# Sample plotting clode (replace successes where necessary)\n","exp_to_accuracies = {\n","    \"bitflip_7_her_none\": success_rate,\n","    \"bitflip_7_her_ final\": success_rate\n","}\n","\n","# Creates the Figure\n","fig = go.Figure()\n","data = []\n","for experiment, accuracies in exp_to_accuracies.items():\n","  steps = range(len(accuracies))\n","  steps = [5 * x for x in steps]\n","  data.append(go.Scatter(x=steps, y=accuracies, line_shape='spline', name=experiment))\n","\n","# Applies a custom layout\n","layout = go.Layout(\n","    title=go.layout.Title(\n","        text='Bitflip with 7 bits',\n","        x=0.5\n","    ),\n","    xaxis=go.layout.XAxis(\n","        title=go.layout.xaxis.Title(\n","            text='Epoch',\n","            font=dict(\n","                family='Courier New, monospace',\n","                size=18,\n","                color='#7f7f7f'\n","            )\n","        )\n","    ),\n","    yaxis=go.layout.YAxis(\n","        title=go.layout.yaxis.Title(\n","            text='Success Rate',\n","            font=dict(\n","                family='Courier New, monospace',\n","                size=18,\n","                color='#7f7f7f'\n","            )\n","        )\n","    )\n",")\n","\n","fig = go.Figure(data=data, layout=layout)\n","\n","HTML(fig.to_html())"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"GqMV9wr0Jdn9"},"source":[""],"execution_count":null,"outputs":[]}]}